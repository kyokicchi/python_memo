{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#基本事項\" data-toc-modified-id=\"基本事項-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>基本事項</a></span><ul class=\"toc-item\"><li><span><a href=\"#コード先頭\" data-toc-modified-id=\"コード先頭-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>コード先頭</a></span></li><li><span><a href=\"#雑多メモ\" data-toc-modified-id=\"雑多メモ-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>雑多メモ</a></span></li><li><span><a href=\"#List-関連\" data-toc-modified-id=\"List-関連-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>List 関連</a></span></li><li><span><a href=\"#Set-関連\" data-toc-modified-id=\"Set-関連-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Set 関連</a></span></li><li><span><a href=\"#Dictionary-関連\" data-toc-modified-id=\"Dictionary-関連-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Dictionary 関連</a></span></li><li><span><a href=\"#Str-関連\" data-toc-modified-id=\"Str-関連-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Str 関連</a></span></li><li><span><a href=\"#Binary-関連\" data-toc-modified-id=\"Binary-関連-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Binary 関連</a></span></li><li><span><a href=\"#Class-使用例\" data-toc-modified-id=\"Class-使用例-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Class 使用例</a></span><ul class=\"toc-item\"><li><span><a href=\"#Class-使用例-1\" data-toc-modified-id=\"Class-使用例-1-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Class 使用例 1</a></span></li><li><span><a href=\"#Class-使用例-2\" data-toc-modified-id=\"Class-使用例-2-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>Class 使用例 2</a></span></li><li><span><a href=\"#Class-使用例-3-承継\" data-toc-modified-id=\"Class-使用例-3-承継-1.8.3\"><span class=\"toc-item-num\">1.8.3&nbsp;&nbsp;</span>Class 使用例 3 承継</a></span></li></ul></li></ul></li><li><span><a href=\"#I/O-関連\" data-toc-modified-id=\"I/O-関連-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>I/O 関連</a></span><ul class=\"toc-item\"><li><span><a href=\"#標準入力受け取り\" data-toc-modified-id=\"標準入力受け取り-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>標準入力受け取り</a></span></li><li><span><a href=\"#ファイル入出力\" data-toc-modified-id=\"ファイル入出力-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>ファイル入出力</a></span></li><li><span><a href=\"#JSON入出力\" data-toc-modified-id=\"JSON入出力-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>JSON入出力</a></span><ul class=\"toc-item\"><li><span><a href=\"#基本\" data-toc-modified-id=\"基本-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>基本</a></span></li><li><span><a href=\"#CSV読みm込み＆JSON展開関数\" data-toc-modified-id=\"CSV読みm込み＆JSON展開関数-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>CSV読みm込み＆JSON展開関数</a></span></li></ul></li><li><span><a href=\"#Pandas-入出力\" data-toc-modified-id=\"Pandas-入出力-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Pandas 入出力</a></span></li><li><span><a href=\"#機械学習-modelオブジェクト-入出力\" data-toc-modified-id=\"機械学習-modelオブジェクト-入出力-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>機械学習 modelオブジェクト 入出力</a></span></li><li><span><a href=\"#Tensorflow-モデル保存-&amp;-読み込み\" data-toc-modified-id=\"Tensorflow-モデル保存-&amp;-読み込み-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Tensorflow モデル保存 &amp; 読み込み</a></span></li><li><span><a href=\"#Google-Colab--Google-Drive-マウント\" data-toc-modified-id=\"Google-Colab--Google-Drive-マウント-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Google Colab  Google Drive マウント</a></span></li></ul></li><li><span><a href=\"#Crawling-関連\" data-toc-modified-id=\"Crawling-関連-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Crawling 関連</a></span><ul class=\"toc-item\"><li><span><a href=\"#simple-crawler\" data-toc-modified-id=\"simple-crawler-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>simple crawler</a></span></li><li><span><a href=\"#xpath-事例\" data-toc-modified-id=\"xpath-事例-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>xpath 事例</a></span></li></ul></li><li><span><a href=\"#Pandas\" data-toc-modified-id=\"Pandas-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pandas</a></span><ul class=\"toc-item\"><li><span><a href=\"#pandas-general\" data-toc-modified-id=\"pandas-general-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>pandas general</a></span></li><li><span><a href=\"#ダミー変数化\" data-toc-modified-id=\"ダミー変数化-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>ダミー変数化</a></span><ul class=\"toc-item\"><li><span><a href=\"#get-dummies\" data-toc-modified-id=\"get-dummies-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>get dummies</a></span></li><li><span><a href=\"#to-categorical\" data-toc-modified-id=\"to-categorical-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>to categorical</a></span></li></ul></li><li><span><a href=\"#カテゴリを数値化\" data-toc-modified-id=\"カテゴリを数値化-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>カテゴリを数値化</a></span><ul class=\"toc-item\"><li><span><a href=\"#factorize\" data-toc-modified-id=\"factorize-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>factorize</a></span></li><li><span><a href=\"#Label-Encoder\" data-toc-modified-id=\"Label-Encoder-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Label Encoder</a></span></li></ul></li><li><span><a href=\"#cut\" data-toc-modified-id=\"cut-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>cut</a></span></li></ul></li><li><span><a href=\"#Matplot\" data-toc-modified-id=\"Matplot-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Matplot</a></span><ul class=\"toc-item\"><li><span><a href=\"#import\" data-toc-modified-id=\"import-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>import</a></span></li><li><span><a href=\"#scatter\" data-toc-modified-id=\"scatter-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>scatter</a></span></li><li><span><a href=\"#bar\" data-toc-modified-id=\"bar-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>bar</a></span></li></ul></li><li><span><a href=\"#Sci-kit-Learn-&amp;-XGBoost\" data-toc-modified-id=\"Sci-kit-Learn-&amp;-XGBoost-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Sci-kit Learn &amp; XGBoost</a></span><ul class=\"toc-item\"><li><span><a href=\"#データセット分割\" data-toc-modified-id=\"データセット分割-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>データセット分割</a></span></li><li><span><a href=\"#Grid-Search-Cross-Validation\" data-toc-modified-id=\"Grid-Search-Cross-Validation-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Grid Search Cross Validation</a></span></li><li><span><a href=\"#各種機械学習モデル\" data-toc-modified-id=\"各種機械学習モデル-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>各種機械学習モデル</a></span><ul class=\"toc-item\"><li><span><a href=\"#MultiLayerPerceptron-Classifier\" data-toc-modified-id=\"MultiLayerPerceptron-Classifier-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;</span>MultiLayerPerceptron Classifier</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-6.3.2\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;</span>Random Forest</a></span></li><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-6.3.3\"><span class=\"toc-item-num\">6.3.3&nbsp;&nbsp;</span>XGBoost</a></span></li></ul></li><li><span><a href=\"#model-比較\" data-toc-modified-id=\"model-比較-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>model 比較</a></span></li><li><span><a href=\"#複数のモデルをざっくり試す\" data-toc-modified-id=\"複数のモデルをざっくり試す-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>複数のモデルをざっくり試す</a></span></li></ul></li><li><span><a href=\"#tensorflow\" data-toc-modified-id=\"tensorflow-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>tensorflow</a></span></li><li><span><a href=\"#Keras\" data-toc-modified-id=\"Keras-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Keras</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本事項"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コード先頭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#jupyter notebook timer\n",
    "\n",
    "%%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 雑多メモ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PrintのFormating\n",
    "print(\"{0} is {1} years old and he is {2}\".format(\"Mike\",35,\"scary\"))\n",
    "print(\"a: %f, b: %f\" %(a , b))\n",
    "\n",
    "#改行なし Print\n",
    "print(text,end='')\n",
    "\n",
    "#Enumerate\n",
    "x=[\"item1\",\"item2\",\"item3\"]\n",
    "for index,item in enumerate(x):\n",
    "    print(index,\":\",item)\n",
    "\n",
    "y={'key1':'val1', 'key2':'val2', 'key3':'val3'}\n",
    "for index,key in enumerate(y):\n",
    "    print(\"{0} : {1} : {2}\".format(index,key,y[key]))\n",
    "\n",
    "#Function 引数の個数指定なし\n",
    "def biggest_number(*args):\n",
    "    return max(args)\n",
    "X = biggest_number(-10, -5, 5, 10)\n",
    "\n",
    "#Counter\n",
    "from collections import Counter\n",
    "c = Counter([1,1,2,3,3,4,5,5,5])\n",
    "print(c.most_common(3))\n",
    "\n",
    "#Datetime\n",
    "import datetime                       \n",
    "everything = dir(datetime)         \n",
    "print(everything)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List 関連"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_list = list(range(1, 11))\n",
    "backwards = my_list[::-1]\n",
    "\n",
    "l_data = [\"score\",\"40\",\"50\",\"80\"]\n",
    "tmp = l_data.pop(0)\n",
    "n_total = sum(map(lambda x: int(x), l_data))\n",
    "\n",
    "evens_to_50 = [x for x in range(51) if x % 2 == 0]\n",
    "even_squares = [x**2 for x in range(1,11) if x % 2 == 0]\n",
    "threes_and_fives = [x for x in range(1,16) if x %3 == 0 or x % 5 == 0]\n",
    "\n",
    "my_list = list(filter(lambda x: x % 3 == 0,range(16)))\n",
    "squares = list(filter(lambda x:x>=30 and x<= 70,[x**2 for x in range(1,11)]))\n",
    "\n",
    "li = [3, 4, 3, 2, 5, 4]\n",
    "li_unique = list(set(li))\n",
    "\n",
    "l = [1,1,2,3,4,4,5,10]\n",
    "if all(x < 10 for x in l):\n",
    "    print(\"All numbers are less than 10\")\n",
    "if any(x >= 10 for x in l):\n",
    "    print(\"There is a number greater than 10\")\n",
    "\n",
    "animals = [\"aardvark\", \"badger\", \"duck\", \"emu\", \"fennec fox\"]\n",
    "duck_index = animals.index(\"duck\")\n",
    "animals.insert(duck_index,\"cobra\")\n",
    "\n",
    "list_a = [3, 9, 17, 15, 19]\n",
    "list_b = [2, 4, 8, 10, 30, 40, 50, 60, 70, 80, 90]\n",
    "for a, b in zip(list_a, list_b):\n",
    "    print(max(a,b), end=\"  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set 関連"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_a = {7,9,2}\n",
    "set_b = {9,22,2,25}\n",
    "set_c = {9,18,21,5}\n",
    "\n",
    "set_b.add(12)\n",
    "set_b.discard(0)\n",
    "\n",
    "set_union = set_a.union(set_b)\n",
    "set_intersection = set_a.intersection(set_b,set_c)\n",
    "set_differences = set_a.difference(set_b,set_c)\n",
    "set_symmetric_difference = set_a.symmetric_difference(set_b)\n",
    "\n",
    "\n",
    "\n",
    "keys = set()\n",
    "for data in df_train:\n",
    "    data_parsed = json.loads(data)\n",
    "    [keys.add(key) for key in data_parsed.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary 関連"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dict = {\"food\":\"apple\", \"tool\":\"spoon\", \"person\":\"jay\"}\n",
    "print(my_dict.items())\n",
    "print(my_dict.keys())\n",
    "print(my_dict.values())\n",
    "\n",
    "candidates = {\"a\":15, \"b\":25, \"c\":35}\n",
    "winnerVote = int(max(candidates.values()))\n",
    "\n",
    "#Sort\n",
    "orig = {\"1\": 3,  \"3\": 1, \"2\": 2}\n",
    "sortByKey = sorted(orig.items())\n",
    "sortByVal = sorted(orig.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Str 関連"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str = \"abcde\"\n",
    "L = list(str)\n",
    "str2 = \"\".join(L)\n",
    "\n",
    "phrase = \"A bird in the hand...\"\n",
    "for c in phrase:\n",
    "    if c.lower() == 'a':\n",
    "        print('X',end='')\n",
    "    else:\n",
    "        print(c,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary 関連"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 0b1001 + 0b1001\n",
    "y = 0b1001 * 0b1001\n",
    "print (int(\"111\",2))\n",
    "print (int(\"0b100\",2))\n",
    "print (bin(5))\n",
    "print (int(bin(5),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 使用例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 使用例 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Student:\n",
    "    def __init__(self, name, grade, age):\n",
    "        self.name = name\n",
    "        self.grade = grade\n",
    "        self.age = age\n",
    "    def __repr__(self):\n",
    "        return repr((self.name, self.grade, self.age))\n",
    "l_obj_students = [\n",
    "    Student('john', 'A', 15),\n",
    "    Student('jane', 'B', 12),\n",
    "    Student('dave', 'B', 10),\n",
    "]\n",
    "\n",
    "# ageでsort\n",
    "print(sorted(l_obj_students, key=attrgetter('age'))) \n",
    "\n",
    "# gradeでsort さらにageでsort\n",
    "print(sorted(l_obj_students, key=attrgetter('grade', 'age')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 使用例 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Animal(object):\n",
    "    is_alive = True\n",
    "    num_Animals = 0\n",
    "    AnimalDict={}\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        Animal.num_Animals += 1\n",
    "        Animal.AnimalDict[name] = self\n",
    "        \n",
    "zebra = Animal(\"Jeffrey\", 2)\n",
    "giraffe = Animal(\"Bruce\", 1)\n",
    "panda = Animal(\"Chad\", 7)\n",
    "\n",
    "print(zebra.name, zebra.age, zebra.is_alive)\n",
    "print(giraffe.name, giraffe.age, giraffe.is_alive)\n",
    "print(panda.name, panda.age, panda.is_alive)\n",
    "print(Animal.num_Animals)\n",
    "print(Animal.AnimalDict)\n",
    "print(Animal.AnimalDict[\"\"Bruce\"\"].age)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 使用例 3 承継"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Shape(object):\n",
    "    def __init__(self, number_of_sides):\n",
    "        self.number_of_sides = number_of_sides\n",
    "    def __repr__(self):\n",
    "        x = \"\"I have \"\" + str(self.number_of_sides) + \"\" sides\"\"\n",
    "        return x\n",
    "\n",
    "class Triangle(Shape):\n",
    "    def __init__(self,side1, side2, side3):\n",
    "        self.side1 = side1\n",
    "        self.side2 = side2\n",
    "        self.side3 = side3\n",
    "    def __repr__(self):\n",
    "        x = \"\"I have \"\" + str(3) + \"\" sides\"\"\n",
    "        return x\n",
    "\n",
    "myShape = Triangle(1,2,3)\n",
    "print(myShape.side1)\n",
    "print(myShape)\n",
    "\n",
    "yourShape = Shape(4)\n",
    "print(yourShape)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O 関連"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 標準入力受け取り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "DATA = sys.stdin.read()\n",
    "DATA = DATA.replace(\"\\n\",\"\")\n",
    "DATA = DATA.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ファイル入出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item = \"content\"\n",
    "f = open(\"output.txt\", \"w\")\n",
    "f.write(item + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON入出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "dic = {'key1':'val1', 'key2':'val2', 'key3':'val3', 'key4':'val4'}\n",
    "#JSON保存\n",
    "f = open('output.json', 'w')\n",
    "json.dump(dic, f)\n",
    "f.close()\n",
    "#JSON読み込み\n",
    "f = open('output.json', 'r')\n",
    "dic = json.load(f)\n",
    "f.close()\n",
    "\n",
    "s = f.read()\n",
    "dic = json.loads(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV読みm込み＆JSON展開関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_df(csv_path, nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, \n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas 入出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Read\n",
    "df = pd.read_csv('raw.csv')\n",
    "df = pd.read_csv('raw.csv', dtype='object', nrows=30, header=None)\n",
    "#Save\n",
    "df.to_csv('modified.csv')\n",
    "df.to_csv('modified.csv', header=False, index=False)\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "pd.io.json.loads(str_jsondata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習 modelオブジェクト 入出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#モデル保存\n",
    "import pickle\n",
    "filename = 'xgb_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "#モデル読み込み\n",
    "import pickle\n",
    "filename = 'xgb_model.sav'\n",
    "model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow モデル保存 & 読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, '../model/test_model')\n",
    "#Read\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, '../model/test_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab  Google Drive マウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd ../content/gdrive/My Drive/projects/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling 関連"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SimpleSpider(orders,recipe,output):\n",
    "    import requests\n",
    "    import lxml.html\n",
    "    f = open(orders, 'r')\n",
    "    dump= f.read()\n",
    "    f.close()\n",
    "    l_url = dump.split('\\n')\n",
    "    l_out = []\n",
    "    for url in l_url:\n",
    "        if url != '':\n",
    "            response = requests.get(url)\n",
    "            html = lxml.html.fromstring(response.content)\n",
    "            l_res = []\n",
    "            l_res = html.xpath(recipe)\n",
    "            l_out.extend(l_res)\n",
    "\n",
    "    csv_out = \"\\n\".join(l_out)\n",
    "    f = open(output, 'w')\n",
    "    f.write(csv_out)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "orders = 'target.txt' #1url per line\n",
    "recipe = '//a[contains(@href, \"entry\") and (starts-with(@href,\"http://\") or starts-with(@href,\"https://\"))]/@href'\n",
    "output = 'results.csv'\n",
    "\n",
    "SimpleSpider(orders,recipe,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xpath 事例 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Requests ------------------------------------------------------------------------------------------------\n",
    "import requests\n",
    "import pandas as pd\n",
    "git_res = requests.get('https://api.github.com/search/repositories?q=language:python+created:2017-07-28&per_page=3')\n",
    "outP = pd.DataFrame(git_res.json()['items'])[:][['language','stargazers_count','git_url','updated_at','created_at']]\n",
    "\n",
    "#Crawling リンク収集 ------------------------------------------------------------------------------------------------\n",
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "s_xpathcode = '//a[(starts-with(@href, \"http://\") or starts-with(@href, \"https://\"))' + \\\n",
    "'and contains(@href, ' + s_domain + ') and not(contains(@href, \".jpg\"))]/@href'\n",
    "\n",
    "s_xpathcode = '//*[@class =\"entry-content\"]/p/descendant::text()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_temp = df.copy() #こうしないと参照コピーになる\n",
    "\n",
    "df.describe()\n",
    "df.info()\n",
    "df.isnull().sum()\n",
    "df.corr()\n",
    "df.shape\n",
    "\n",
    "df = df.fillna(0)\n",
    "df = pd.concat((df, df_x), axis = 1)\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicated()\n",
    "\n",
    "df = df['ColumnName'].unique()\n",
    "cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]\n",
    "df = df['ColumnName'].value_counts()\n",
    "\n",
    "df = df.drop('ColumnName',axis = 1)\n",
    "df = df.drop('ColumnName',axis = 1, inplace=True)\n",
    "\n",
    "df = df['ColumnName'].str.split(',', expand=True)\n",
    "\n",
    "df['ColumnName'] = df['ColumnName'].notnull() * 1\n",
    "\n",
    "#-------------------Pandas title分類\n",
    "choice = [\"Miss\",\"Mr\",\"Mrs\",\"Master\"]\n",
    "l_title = [list(set(x).intersection(set(choice)))[0] for x in l_names]     # [\"Mr\",\"Johnny\",\"Depp\"] -> \"Mr\"\n",
    "\n",
    "\n",
    "df_x = pd.Series(l_x, name = \"ColumnName\")\n",
    "l_names = df.Name.values.tolist()\n",
    "\n",
    "n_oldest = df.Age.max()\n",
    "df_x = df[df.Age==n_oldest]\n",
    "df_x = df[df.Age>=40]\n",
    "n_family = df.Family.apply(lambda x: (df.Family == x).sum())\n",
    "\n",
    "df = df[df[0].str.contains('//www.target.com')]\n",
    "\n",
    "a = df.Sex == 'male'\n",
    "a.sum()\n",
    "\n",
    "cabin_known_ratio = len(df[((df.Cabin.isnull()==False) & (df.Survived == 1))]) / len(df[df.Cabin.isnull()==False])\n",
    "cabin_unknown_ratio = len(df[((df.Cabin.isnull()) & (df.Survived == 1))]) / len(df[df.Cabin.isnull()])\n",
    "\n",
    "#------------------NaN 部分のみを差し替え\n",
    "    df_res_pred = model.predict(X)\n",
    "    df_real_age = df['Age'].fillna(0)\n",
    "    l_mix = []\n",
    "    for i,v in enumerate(df_real_age):\n",
    "        if v != 0:\n",
    "            l_mix.append(v)\n",
    "        else:\n",
    "            l_mix.append(df_res_pred[i])\n",
    "\n",
    "    df_mix = pd.Series(l_mix, name='Age')\n",
    "    df = df.drop('Age',axis = 1)\n",
    "    df = pd.concat((df,df_mix),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ダミー変数化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummylize(df,n_item):\n",
    "    dum = pd.get_dummies(df[n_item], drop_first = True)\n",
    "    df = pd.concat((df, dum),axis = 1)\n",
    "    df = df.drop(n_item,axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype('int32'),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カテゴリを数値化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize(df,n_item):\n",
    "    labels, uniques = pd.factorize(df[n_item])\n",
    "    df[n_item] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "num_cat = le.fit_transform(str_categories)\n",
    "df_n_cat = pd.Series(num_cat, name='NumCat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainデータセットとtestデータセット共通のルールでCategoryを数値化\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in category_cols:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(df_train[col].values.astype('str')) + list(df_test[col].values.astype('str')))\n",
    "    df_train[col] = lbl.transform(list(df_train[col].values.astype('str')))\n",
    "    df_test[col] = lbl.transform(list(df_test[col].values.astype('str')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.cut(df, [0, 10, 50, 100])\n",
    "pd.cut(df, 4, right=False)\n",
    "counts = pd.cut(df, 3, labels=['S', 'M', 'L']).value_counts()\n",
    "df['Age_bin'] = pd.cut(df['Age'], 5, labels=False)\n",
    "\n",
    "s_cut, bins = pd.cut(df, 4, retbins=True)\n",
    "print(s_cut)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "df_draw = df_tmp[df_tmp.Survived==1]\n",
    "df_age = df_draw.iloc[:,2]\n",
    "df_sex = df_draw.iloc[:,6]\n",
    "plt.scatter(df_age,df_sex, color='#cc6699',alpha=0.5)\n",
    "df_draw = df_tmp[df_tmp.Survived==0]\n",
    "df_age = df_draw.iloc[:,2]\n",
    "df_sex = df_draw.iloc[:,6]\n",
    "plt.scatter(df_age,df_sex, color='#6699cc',alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sex_survived_ratio = [male_survived_ratio, female_survived_ratio]\n",
    "plt.bar([0,1], sex_survived_ratio, tick_label=['male', 'female'], width=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sci-kit Learn & XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセット分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l_features = ['Pclass','male','Q','S','TitlesNum','FamilyNum','Cabin','Age','SibSp','Parch','Fare']\n",
    "df_x = df_train[l_features]\n",
    "df_y = df_train.Survived\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def applyGSCV(model, param, X, Y):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    res = GridSearchCV(model, param, cv=3)\n",
    "    res.fit(X, Y)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各種機械学習モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiLayerPerceptron Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model =  MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "obj_param = { 'n_estimators': [5,10,20,30,50,100,300], 'max_depth': [3,5,10,15,20,25,30,40,50,100] }\n",
    "model = applyGSCV(RandomForestClassifier(),obj_param,X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_model(X, Y):\n",
    "\n",
    "    import xgboost as xgb\n",
    "    import scipy.stats as st\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    one_to_left = st.beta(10, 1)\n",
    "    from_zero_positive = st.expon(0, 50)\n",
    "    params = {\n",
    "        \"n_estimators\": st.randint(3, 40),\n",
    "        \"max_depth\": st.randint(3, 40),\n",
    "        \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "        \"colsample_bytree\": one_to_left,\n",
    "        \"subsample\": one_to_left,\n",
    "        \"gamma\": st.uniform(0, 10),\n",
    "        'reg_alpha': from_zero_positive,\n",
    "        \"min_child_weight\": from_zero_positive,\n",
    "    }\n",
    "    xgbreg = xgb.XGBRegressor(nthreads=-1)\n",
    "    rscv = RandomizedSearchCV(xgbreg, params, n_jobs=3)\n",
    "    res = rscv.fit(X, Y)\n",
    "    return res\n",
    "\n",
    "model = xgb_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "models =[]\n",
    "models.append(model_xgb)\n",
    "models.append(model_randForest)\n",
    "models.append(model_MLPC)\n",
    "\n",
    "for model in models:    \n",
    "    res_tmp = model.predict(X_test)\n",
    "    res_mean = mean_squared_error(y_test, res_tmp)\n",
    "\n",
    "    print('mean squared error:{0} '.format(res_mean))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 複数のモデルをざっくり試す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "names = ['LogisticRegression', 'SVC', 'LinearSVC', 'KNeighbors', 'DecisionTree', 'RandomForest', 'MLPClassifier']\n",
    "l_models = []\n",
    "l_models.append((\"LogisticRegression\", LogisticRegression()))\n",
    "l_models.append((\"SVC\", SVC()))\n",
    "l_models.append((\"LinearSVC\", LinearSVC()))\n",
    "l_models.append((\"KNeighbors\", KNeighborsClassifier()))\n",
    "l_models.append((\"DecisionTree\", DecisionTreeClassifier()))\n",
    "l_models.append((\"RandomForest\", RandomForestClassifier()))\n",
    "l_models.append((\"MLPClassifier\", MLPClassifier(solver='lbfgs', random_state=0)))\n",
    "\n",
    "def evaluate_models(df_forAgePred, l_pred, l_models):\n",
    "    results = []\n",
    "    names = []\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_forAgePred[l_pred], df_forAgePred.Age, test_size=0.25)\n",
    "    for name, model in l_models:\n",
    "        model.fit(x_train, y_train)\n",
    "        res_pred = model.predict(x_test)\n",
    "        result = mean_squared_error(y_test, res_pred)\n",
    "        names.append(name)\n",
    "        results.append(result)\n",
    "    return names, results\n",
    "\n",
    "\n",
    "import statistics\n",
    "result_list = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    print('test {0}'.format(i))\n",
    "    name, res = evaluate_models(df_forAgePred, l_pred, l_models)\n",
    "    for x, y in zip(name, res):\n",
    "        result_list.append([x, y])\n",
    "print('mean squared error results by model')\n",
    "\n",
    "for n in names:\n",
    "    r = [i[1] for i in result_list if i[0] == n]\n",
    "    print(n)\n",
    "    print('avg: {0:,.2f} / median: {1:,.2f}'.format(sum(r)/len(r), statistics.median(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorflow simple 使用例 --------------------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "\n",
    "# Model parameters (variables of function to be learned via machine learning)\n",
    "W = tf.Variable([0.], dtype=tf.float32)\n",
    "b = tf.Variable([0.], dtype=tf.float32)\n",
    "\n",
    "# Model input and output (sets of data and result)\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model itself\n",
    "linear_model = W*x + b\n",
    "\n",
    "# loss\n",
    "deltas = linear_model - y\n",
    "square_deltas = tf.square(deltas)\n",
    "loss = tf.reduce_sum(square_deltas) # sum of the squares\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_dataset = [1, 2, 3, 4]\n",
    "y_dataset = [0, -1, -2, -3]\n",
    "\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) \n",
    "for x in range(1000):\n",
    "  sess.run(train, {x: x_dataset, y: y_dataset})\n",
    "    if (x+1) % 100 == 0:\n",
    "        print('\\nStep: %s' % (x+1))\n",
    "        print('loss: ' + str(sess.run(train, {x: x_dataset, y: y_dataset})))\n",
    "        print(\"W: %f, b: %f\" % (sess.run( W ), sess.run( b )) )\n",
    "\n",
    "# evaluate training accuracy\n",
    "ans_W, ans_b, ans_loss = sess.run([W, b, loss], {x: x_dataset, y: y_dataset})\n",
    "print(\"W: %s b: %s loss: %s\"%(ans_W, ans_b, ans_loss))\n",
    "\n",
    "# Tensorflow Estimator 使用例 ------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "num_columns = [tf.feature_column.numeric_column(\"x\",shape=[1])]\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=num_columns)\n",
    "\n",
    "x_dataset = np.array([1.,2.,3.,4.])\n",
    "y_dataset = np.array([0.,-1.,-2.,-3.])\n",
    "x_eval = np.array([2.,5.,8.,1.])\n",
    "y_eval = np.array([-1.01,-4.1,-7.,0.])\n",
    "\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn({\"x\":x_dataset},y_dataset,batch_size=4, num_epochs=None, shuffle=True)\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn({\"x\":x_dataset},y_dataset,batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn({\"x\":x_eval},y_eval,batch_size=4, num_epochs=1000, shuffle=False)\n",
    "ans_train_data = estimator.evaluate(input_fn=train_input_fn)\n",
    "ans_eval_data = estimator.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "print(\"train data vs model: %r\" % ans_train_data)\n",
    "print(\"eval data vs model: %r\" % ans_eval_data)\n",
    "\n",
    "# tensorflow MNIST example -------------------------------------------------------------------------------------\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "#variables\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#model formula\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "#cross entropy\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "#run\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "#accuracy check\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"accuracy:\", sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keras simple NNモデル---------------------------------------------------------------------------------------\n",
    "import keras.optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "l_features = ['feat1','feat2','feat3','feat4']\n",
    "N = len(l_features)\n",
    "x_train = df_train[l_features]\n",
    "x_test = df_test[l_features]\n",
    "y_train_bin = to_categorical(df_train['ColumnName'])\n",
    "y_test_bin = to_categorical(df_test['ColumnName'])\n",
    "\n",
    "model_NN = Sequential()\n",
    "model_NN.add(Dense(2,input_dim=N, activation='sigmoid', kernel_initializer='uniform'))\n",
    "model_NN.add(Dense(2,activation='softmax', kernel_initializer='uniform'))\n",
    "sgd = keras.optimizers.SGD(lr = 0.5, momentum = 0.0,decay = 0.0, nesterov = False)\n",
    "model_NN.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model_NN.fit(x_train, y_train_bin, batch_size=100,epochs=1000,verbose=0,validation_data=(x_test, y_test_bin))\n",
    "history.history['acc'] #対trainデータ正確性\n",
    "history.history['val_acc'] #対testデータ正確性\n",
    "\n",
    "score = model_NN.evaluate(X_test, y_test_bin, verbose = 0) \n",
    "print(score[0], score[1]) #score[0]が交差エントロピー誤差、score[1]がテストデータ正答率\n",
    "\n",
    "#Keras mnist 使用例----------------------------------------------------------------------------------\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sequential Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Download MNIST datasets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# show sample data\n",
    "fig = plt.figure(figsize=(9 , 9))\n",
    "for i in range(36):\n",
    "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(x_train[i], cmap='gist_gray')\n",
    "plt.show()\n",
    "\n",
    "# reshape 28*28 pixel data into 784 dim data\n",
    "# convert into float type and normalize pixel data from 0.0 to 1.0\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') /255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') /255\n",
    "\n",
    "# encode label data into \"one-hot\"\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype('int32'), 10)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test.astype('int32'), 10)\n",
    "\n",
    "# select Sequiential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st layer : fully connected layer(output:512)\n",
    "# only first layer needs to define input_shape\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# use Dropout regularization rate to avoid overfitting\n",
    "# Randomly ignoring connections between layers\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 2nd layer : fully connected layer(output:512)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 3rd layer : fully connected layer(output:10)\n",
    "# acrivation methods: softmax, which squashes the outputs of each unit to be between 0 and 1.(often used in the final layer)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Set definitions for traning\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "\n",
    "# Excute training for 20(epochs) times\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# plot results\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Keras mnist 使用例 2 ----------------------------------------------------------------------------------\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "img_rows, img_cols = 28, 28\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = y_train.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test =  keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
