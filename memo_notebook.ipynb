{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misc notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "%%time\n",
    "\n",
    "#debugger\n",
    "from IPython import embed\n",
    "#insert below where want to pause and debug\n",
    "embed()\n",
    "\n",
    "#Prints\n",
    "print(\"{0} is {1} years old and he is {2}\".format(\"Mike\",35,\"scary\"))\n",
    "print(\"a: %f, b: %f\" %(a , b))\n",
    "print(text,end='')\n",
    "\n",
    "#Enumerate\n",
    "x=[\"item1\",\"item2\",\"item3\"]\n",
    "for index,item in enumerate(x):\n",
    "    print(index,\":\",item)\n",
    "\n",
    "y={'key1':'val1', 'key2':'val2', 'key3':'val3'}\n",
    "for index,key in enumerate(y):\n",
    "    print(\"{0} : {1} : {2}\".format(index,key,y[key]))\n",
    "\n",
    "#Function 引数の個数指定なし\n",
    "def biggest_number(*args):\n",
    "    return max(args)\n",
    "X = biggest_number(-10, -5, 5, 10)\n",
    "\n",
    "#Counter\n",
    "from collections import Counter\n",
    "c = Counter([1,1,2,3,3,4,5,5,5])\n",
    "print(c.most_common(3))\n",
    "\n",
    "#Datetime\n",
    "import datetime                       \n",
    "everything = dir(datetime)         \n",
    "print(everything)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backwards = my_list[::-1]\n",
    "\n",
    "l_data = [\"score\",\"40\",\"50\",\"80\"]\n",
    "sum_from_list = sum(map(lambda x: int(x), l_data))\n",
    "\n",
    "evens_to_50 = [x for x in range(51) if x % 2 == 0]\n",
    "even_squares = [x**2 for x in range(1,11) if x % 2 == 0]\n",
    "threes_and_fives = [x for x in range(1,16) if x %3 == 0 or x % 5 == 0]\n",
    "my_list = list(filter(lambda x: x % 3 == 0,range(16)))\n",
    "squares = list(filter(lambda x:x>=30 and x<= 70,[x**2 for x in range(1,11)]))\n",
    "\n",
    "l_unique = list(set([3, 4, 3, 2, 5, 4]))\n",
    "\n",
    "if all(x < 10 for x in [1,1,2,3,4,4,5,10]):\n",
    "    print(\"All numbers are less than 10\")\n",
    "if any(x >= 10 for x in [1,1,2,3,4,4,5,10]):\n",
    "    print(\"There is a number greater than 10\")\n",
    "\n",
    "animals = [\"aardvark\", \"badger\", \"duck\", \"emu\", \"fennec fox\"]\n",
    "duck_index = animals.index(\"duck\")\n",
    "animals.insert(duck_index,\"cobra\")\n",
    "\n",
    "list_a = [3, 9, 17, 15, 19]\n",
    "list_b = [2, 4, 8, 10, 30, 40, 50, 60, 70, 80, 90]\n",
    "for a, b in zip(list_a, list_b):\n",
    "    print(max(a,b), end=\"  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_a = {7,9,2}\n",
    "set_b = {9,22,2,25}\n",
    "set_c = {9,18,21,5}\n",
    "\n",
    "set_b.add(12)\n",
    "set_b.discard(0)\n",
    "\n",
    "set_union = set_a.union(set_b)\n",
    "set_intersection = set_a.intersection(set_b,set_c)\n",
    "set_differences = set_a.difference(set_b,set_c)\n",
    "set_symmetric_difference = set_a.symmetric_difference(set_b)\n",
    "\n",
    "keys = set()\n",
    "for data in df_train:\n",
    "    data_parsed = json.loads(data)\n",
    "    [keys.add(key) for key in data_parsed.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = dict(food='apple',tool='spoon',person='jay', count=4)\n",
    "print(my_dict.items())\n",
    "print(my_dict.keys())\n",
    "print(my_dict.values())\n",
    "\n",
    "candidates = {\"a\":15, \"b\":25, \"c\":35}\n",
    "winnerVote = int(max(candidates.values()))\n",
    "\n",
    "#Sort\n",
    "orig = {\"1\": 3,  \"3\": 1, \"2\": 2}\n",
    "sortByKey = sorted(orig.items())\n",
    "sortByVal = sorted(orig.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"abcde\"\n",
    "L = list(str)\n",
    "str2 = \"\".join(L)\n",
    "\n",
    "phrase = \"A bird in the hand...\"\n",
    "for c in phrase:\n",
    "    if c.lower() == 'a':\n",
    "        print('X',end='')\n",
    "    else:\n",
    "        print(c,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0b1001 + 0b1001\n",
    "y = 0b1001 * 0b1001\n",
    "print (int(\"111\",2))\n",
    "print (int(\"0b100\",2))\n",
    "print (bin(5))\n",
    "print (int(bin(5),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student:\n",
    "    def __init__(self, name, grade, age):\n",
    "        self.name = name\n",
    "        self.grade = grade\n",
    "        self.age = age\n",
    "    def __repr__(self):\n",
    "        return repr((self.name, self.grade, self.age))\n",
    "l_obj_students = [\n",
    "    Student('john', 'A', 15),\n",
    "    Student('jane', 'B', 12),\n",
    "    Student('dave', 'B', 10),\n",
    "]\n",
    "\n",
    "# ageでsort\n",
    "print(sorted(l_obj_students, key=attrgetter('age'))) \n",
    "\n",
    "# gradeでsort さらにageでsort\n",
    "print(sorted(l_obj_students, key=attrgetter('grade', 'age')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animal(object):\n",
    "    is_alive = True\n",
    "    num_Animals = 0\n",
    "    AnimalDict={}\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        Animal.num_Animals += 1\n",
    "        Animal.AnimalDict[name] = self\n",
    "        \n",
    "zebra = Animal(\"Jeffrey\", 2)\n",
    "giraffe = Animal(\"Bruce\", 1)\n",
    "panda = Animal(\"Chad\", 7)\n",
    "\n",
    "print(zebra.name, zebra.age, zebra.is_alive)\n",
    "print(Animal.num_Animals)\n",
    "print(Animal.AnimalDict)\n",
    "print(Animal.AnimalDict[\"\"Bruce\"\"].age)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shape(object):\n",
    "    def __init__(self, number_of_sides):\n",
    "        self.number_of_sides = number_of_sides\n",
    "    def __repr__(self):\n",
    "        x = \"\"I have \"\" + str(self.number_of_sides) + \"\" sides\"\"\n",
    "        return x\n",
    "\n",
    "class Triangle(Shape):\n",
    "    def __init__(self,side1, side2, side3):\n",
    "        self.side1 = side1\n",
    "        self.side2 = side2\n",
    "        self.side3 = side3\n",
    "    def __repr__(self):\n",
    "        x = \"\"I have \"\" + str(3) + \"\" sides\"\"\n",
    "        return x\n",
    "\n",
    "myShape = Triangle(1,2,3)\n",
    "print(myShape.side1)\n",
    "print(myShape)\n",
    "\n",
    "yourShape = Shape(4)\n",
    "print(yourShape)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## std input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "DATA = sys.stdin.read()\n",
    "DATA = DATA.replace(\"\\n\",\"\")\n",
    "DATA = DATA.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## file I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = \"content\"\n",
    "f = open(\"output.txt\", \"w\")\n",
    "f.write(item + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dic = {'key1':'val1', 'key2':'val2', 'key3':'val3', 'key4':'val4'}\n",
    "#JSON保存\n",
    "f = open('output.json', 'w')\n",
    "json.dump(dic, f)\n",
    "f.close()\n",
    "#JSON読み込み\n",
    "f = open('output.json', 'r')\n",
    "dic = json.load(f)\n",
    "f.close()\n",
    "\n",
    "s = f.read()\n",
    "dic = json.loads(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# google colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd ../content/gdrive/My Drive/projects/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for enabling cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upload files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "csv_data = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from google.colab import files\n",
    "filename = 'model_LGB.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "files.download(filename) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas & feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#copy\n",
    "df_new = df.copy() #こうしないと参照コピーになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list <- -> dataframe\n",
    "df_x = pd.Series(l_x, name = \"ColumnName\")\n",
    "l_x = df['ColumnName'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((df, df_x), axis = 1)\n",
    "df = df.drop('ColumnName',axis = 1)\n",
    "df.drop('ColumnName',axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df.apply(), df.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = [\"Miss\",\"Mr\",\"Mrs\",\"Master\"]\n",
    "l_title = [list(set(x).intersection(set(choice)))[0] for x in l_names]     # [\"Mr\",\"Johnny\",\"Depp\"] -> \"Mr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Save\n",
    "df.to_csv('modified.csv')\n",
    "df.to_csv('modified.csv', header=False, index=False)\n",
    "\n",
    "#Load\n",
    "df = pd.read_csv('raw.csv')\n",
    "df = pd.read_csv('raw.csv',index_col=0, dtype='object', nrows=30, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "\n",
    "def load_df(csv_path, nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "        df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, \n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "df.info()\n",
    "df.corr()\n",
    "\n",
    "df.head()\n",
    "df.tail()\n",
    "\n",
    "df.dtypes \n",
    "df.shape\n",
    "\n",
    "df.isnull().sum()\n",
    "df.notnull().sum()\n",
    "\n",
    "df['ColumnName'].value_counts()\n",
    "df['ColumnName'].unique()\n",
    "df['ColumnName'].nunique()\n",
    "len(df[df<=0])\n",
    "\n",
    "df.select_dtypes('object').apply(pd.Series.nunique,axis=0)\n",
    "\n",
    "df_tmp = df.select_dtypes(exclude=np.float)\n",
    "\n",
    "df = df[df['ColumnOne'].str.contains('www.')]\n",
    "\n",
    "df_tmp = df.query(\"ColumnOne==1 and ColumnTwo >= 30\")\n",
    "df_tmp = df.filter(regex='^P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## align # of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df_train.align(df_test, join='outer', axis=1)\n",
    "\n",
    "df_train.replace(to_replace=np.nan, value=0, inplace=True)\n",
    "df_test.replace(to_replace=np.nan, value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop / fill columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicated()\n",
    "df['ColumnName'] = df['ColumnName'].notnull() * 1\n",
    "\n",
    "df = df.fillna(0)\n",
    "df[col].fillna(df[col].mode()[0],inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_touch = ['ProtectColumn1','ProtectColumn2']\n",
    "\n",
    "cols_not_unique = [col for col in df_train.columns if df_train[col].nunique() < 2]\n",
    "\n",
    "cols_null_ratio_high = [col for col in df_train.columns if sum(df_train[col].isnull()) / len(df_train) >= 0.6]\n",
    "\n",
    "cols_not_much_var = []\n",
    "df_num = df_train.select_dtypes(include=[np.number])\n",
    "df_var = df_num.var()\n",
    "for x in df_num:\n",
    "    if df_var[x] <= 30:\n",
    "        cols_not_much_var.append(x)\n",
    "\n",
    "\n",
    "del_cols = []\n",
    "del_cols.extend(cols_not_unique)\n",
    "del_cols.extend(cols_null_ratio_high)\n",
    "del_cols.extend(cols_not_much_var)\n",
    "del_cols = list(set(del_cols))\n",
    "\n",
    "del_cols = [x for x in del_cols if x not in no_touch]\n",
    "\n",
    "df_train.drop(del_cols, axis=1, inplace=True)\n",
    "df_test.drop(del_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deal with dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix     \n",
    "df['date'] = pd.to_datetime(df['date'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))\n",
    "\n",
    "#extract\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekday'] = df['date'].dt.weekday\n",
    "df['weekofyear'] = df['date'].dt.weekofyear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cnt_GrpDate'] = df.groupby(['Group','Date'])['TargetColumn'].transform('count')\n",
    "\n",
    "df['sum_GrpDate'] = df.groupby(['Group','Date'])['TargetColumn'].transform('sum')\n",
    "\n",
    "df['avg_GrpDate'] = df.groupby(['Group','Date'])['TargetColumn'].transform('mean')\n",
    "\n",
    "df['max_GrpDate'] = df.groupby(['Group','Date'])['TargetColumn'].transform('max')\n",
    "\n",
    "df['min_GrpDate'] = df.groupby(['Group','Date'])['TargetColumn'].transform('min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('GroupingColumn')['TargetColumn'].value_counts().unstack()\n",
    "df.pivot_table(index='GroupingColumnOne', columns='GroupingColumnTwo', values='TargetColumn', aggfunc=np.mean)\n",
    "\n",
    "df = df['ColumnName'].str.split(',', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AGG(df,cols_grp,agg_rules,):\n",
    "\n",
    "    df_agg = df.groupby(cols_grp).agg(aggs_rules)\n",
    "\n",
    "    new_columns = [ key + '_' + rule for key in agg_rules.keys() for rule in aggs[k] ] # 'ColumnOne_min' \n",
    "    df_agg.columns = new_columns    \n",
    "    return df_agg\n",
    "\n",
    "cols_grp = ['ColumnToGroupByOne','ColumnToGroupByTwo']\n",
    "\n",
    "agg_rules = {\n",
    "    'ColumnOne': ['min', 'max'],\n",
    "    'ColumnTwo': ['sum', 'size'],\n",
    "    'ColumnFour': ['sum', 'min', 'max', 'mean', 'median'],\n",
    "    'ColumnFive': ['sum', 'mean', 'median']\n",
    "    'ColumnSix': ['min', 'max', 'mean', 'median', 'var', 'std']\n",
    "}\n",
    "\n",
    "df_new = AGG(df,s_groupby,agg_rules,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(df, [0, 10, 50, 100])\n",
    "pd.cut(df, 4, right=False)\n",
    "counts = pd.cut(df, 3, labels=['S', 'M', 'L']).value_counts()\n",
    "df['Age_bin'] = pd.cut(df['Age'], 5, labels=False)\n",
    "\n",
    "s_cut, bins = pd.cut(df, 4, retbins=True)\n",
    "print(s_cut)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas get dummies\n",
    "def dummylize(df,n_item):\n",
    "    dum = pd.get_dummies(df[n_item], drop_first = True)\n",
    "    df = pd.concat((df, dum),axis = 1)\n",
    "    df = df.drop(n_item,axis = 1)\n",
    "    return df\n",
    "\n",
    "#numpy to categorical\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype('int32'),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## category to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in category_cols:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(df_train[col].values.astype('str')) + list(df_test[col].values.astype('str')))\n",
    "    df_train[col] = lbl.transform(list(df_train[col].values.astype('str')))\n",
    "    df_test[col] = lbl.transform(list(df_test[col].values.astype('str')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "l_tgt = ['ColmunOne','ColumnTwo']\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "#scaler = preprocessing.StandardScaler()\n",
    "\n",
    "df_train[l_tgt] = scaler.fit_transform(df_train[l_tgt])\n",
    "df_test[l_tgt] = scaler.transform(df_test[l_tgt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine learning basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load / save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "\n",
    "import pickle\n",
    "filename = 'xgb_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "#load\n",
    "\n",
    "import pickle\n",
    "filename = 'xgb_model.sav'\n",
    "model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict, ennsamble and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.sav'\n",
    "model = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict(df_x)\n",
    "mse = mean_squared_error(df_y, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tgt = model.predict(df_tgt)\n",
    "\n",
    "pred_tgt[pred_tgt<0] = 0\n",
    "df_out = pd.DataFrame(index = df_tgt.index)\n",
    "df_out['PredictedLogRevenue'] = pred_tgt\n",
    "df_out.to_csv('output_LGB.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = ()\n",
    "param = {}\n",
    "\n",
    "def applyGSCV(model, param, X, Y):\n",
    "    res = GridSearchCV(model, param, cv=3)\n",
    "    res.fit(X, Y)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "models =[]\n",
    "models.append(model_xgb)\n",
    "models.append(model_randForest)\n",
    "models.append(model_MLPC)\n",
    "\n",
    "for model in models:    \n",
    "    res_tmp = model.predict(X_test)\n",
    "    res_mean = mean_squared_error(y_test, res_tmp)\n",
    "\n",
    "    print('mean squared error:{0} '.format(res_mean))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "df_x = df.drop(['TARGET'], axis = 1, inplace = True)\n",
    "df_y = df['TARGET']\n",
    "model = RandomForestRegressor(max_depth=10)\n",
    "model.fit(df_x, df_y)\n",
    "\n",
    "imp = model.feature_importances_\n",
    "sample = np.argsort(imp[0:20])  # top 20 features\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(imp_show)), imp[sample], color='b', align='center')\n",
    "plt.yticks(range(len(imp_show)), [df.columns[x] for x in sample])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "names = ['LogisticRegression', 'SVC', 'LinearSVC', 'KNeighbors', 'DecisionTree', 'RandomForest', 'MLPClassifier']\n",
    "l_models = []\n",
    "l_models.append((\"LogisticRegression\", LogisticRegression()))\n",
    "l_models.append((\"SVC\", SVC()))\n",
    "l_models.append((\"LinearSVC\", LinearSVC()))\n",
    "l_models.append((\"KNeighbors\", KNeighborsClassifier()))\n",
    "l_models.append((\"DecisionTree\", DecisionTreeClassifier()))\n",
    "l_models.append((\"RandomForest\", RandomForestClassifier()))\n",
    "l_models.append((\"MLPClassifier\", MLPClassifier(solver='lbfgs', random_state=0)))\n",
    "\n",
    "def evaluate_models(df_forAgePred, l_pred, l_models):\n",
    "    results = []\n",
    "    names = []\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_forAgePred[l_pred], df_forAgePred.Age, test_size=0.25)\n",
    "    for name, model in l_models:\n",
    "        model.fit(x_train, y_train)\n",
    "        res_pred = model.predict(x_test)\n",
    "        result = mean_squared_error(y_test, res_pred)\n",
    "        names.append(name)\n",
    "        results.append(result)\n",
    "    return names, results\n",
    "\n",
    "\n",
    "import statistics\n",
    "result_list = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    print('test {0}'.format(i))\n",
    "    name, res = evaluate_models(df_forAgePred, l_pred, l_models)\n",
    "    for x, y in zip(name, res):\n",
    "        result_list.append([x, y])\n",
    "print('mean squared error results by model')\n",
    "\n",
    "for n in names:\n",
    "    r = [i[1] for i in result_list if i[0] == n]\n",
    "    print(n)\n",
    "    print('avg: {0:,.2f} / median: {1:,.2f}'.format(sum(r)/len(r), statistics.median(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 200,\n",
    "    'num_leaves': 128,\n",
    "    'subsample': 0.2217,\n",
    "    'colsample_bytree': 0.6810,\n",
    "    'min_split_gain': np.power(10.0, -4.9380),\n",
    "    'reg_alpha': np.power(10.0, -3.2454),\n",
    "    'reg_lambda': np.power(10.0, -4.8571),\n",
    "    'min_child_weight': np.power(10.0, 2),\n",
    "    'silent': True\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(x_train, y_train,\n",
    "          eval_set=[(x_train, y_train),(x_test, y_test)],\n",
    "          eval_metric='rmse',\n",
    "          early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 25))\n",
    "lgb.plot_importance(model, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    ")\n",
    "\n",
    "params = {\n",
    " 'max_depth':[x for x in range(3,10,2)],\n",
    " 'min_child_weight':[x for x in range(1,6,2)]\n",
    "}\n",
    "\n",
    "cv = model_selection.GridSearchCV(estimator = model, param_grid = params, n_jobs=-1, cv=3, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.best_params_, cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgb.XGBRegressor(**cv.best_params_)\n",
    "model.fit(df_x, df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 25))\n",
    "xgb.plot_importance(model, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras simple NNモデル---------------------------------------------------------------------------------------\n",
    "import keras.optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "l_features = ['feat1','feat2','feat3','feat4']\n",
    "N = len(l_features)\n",
    "x_train = df_train[l_features]\n",
    "x_test = df_test[l_features]\n",
    "y_train_bin = to_categorical(df_train['ColumnName'])\n",
    "y_test_bin = to_categorical(df_test['ColumnName'])\n",
    "\n",
    "model_NN = Sequential()\n",
    "model_NN.add(Dense(2,input_dim=N, activation='sigmoid', kernel_initializer='uniform'))\n",
    "model_NN.add(Dense(2,activation='softmax', kernel_initializer='uniform'))\n",
    "sgd = keras.optimizers.SGD(lr = 0.5, momentum = 0.0,decay = 0.0, nesterov = False)\n",
    "model_NN.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model_NN.fit(x_train, y_train_bin, batch_size=100,epochs=1000,verbose=0,validation_data=(x_test, y_test_bin))\n",
    "history.history['acc'] #対trainデータ正確性\n",
    "history.history['val_acc'] #対testデータ正確性\n",
    "\n",
    "score = model_NN.evaluate(X_test, y_test_bin, verbose = 0) \n",
    "print(score[0], score[1]) #score[0]が交差エントロピー誤差、score[1]がテストデータ正答率\n",
    "\n",
    "#Keras mnist 使用例----------------------------------------------------------------------------------\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sequential Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Download MNIST datasets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# show sample data\n",
    "fig = plt.figure(figsize=(9 , 9))\n",
    "for i in range(36):\n",
    "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(x_train[i], cmap='gist_gray')\n",
    "plt.show()\n",
    "\n",
    "# reshape 28*28 pixel data into 784 dim data\n",
    "# convert into float type and normalize pixel data from 0.0 to 1.0\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') /255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') /255\n",
    "\n",
    "# encode label data into \"one-hot\"\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype('int32'), 10)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test.astype('int32'), 10)\n",
    "\n",
    "# select Sequiential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st layer : fully connected layer(output:512)\n",
    "# only first layer needs to define input_shape\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# use Dropout regularization rate to avoid overfitting\n",
    "# Randomly ignoring connections between layers\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 2nd layer : fully connected layer(output:512)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 3rd layer : fully connected layer(output:10)\n",
    "# acrivation methods: softmax, which squashes the outputs of each unit to be between 0 and 1.(often used in the final layer)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Set definitions for traning\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "\n",
    "# Excute training for 20(epochs) times\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# plot results\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Keras mnist 使用例 2 ----------------------------------------------------------------------------------\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "img_rows, img_cols = 28, 28\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = y_train.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test =  keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, '../model/test_model')\n",
    "\n",
    "#load\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, '../model/test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow simple 使用例 --------------------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "\n",
    "# Model parameters (variables of function to be learned via machine learning)\n",
    "W = tf.Variable([0.], dtype=tf.float32)\n",
    "b = tf.Variable([0.], dtype=tf.float32)\n",
    "\n",
    "# Model input and output (sets of data and result)\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model itself\n",
    "linear_model = W*x + b\n",
    "\n",
    "# loss\n",
    "deltas = linear_model - y\n",
    "square_deltas = tf.square(deltas)\n",
    "loss = tf.reduce_sum(square_deltas) # sum of the squares\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_dataset = [1, 2, 3, 4]\n",
    "y_dataset = [0, -1, -2, -3]\n",
    "\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) \n",
    "for x in range(1000):\n",
    "  sess.run(train, {x: x_dataset, y: y_dataset})\n",
    "    if (x+1) % 100 == 0:\n",
    "        print('\\nStep: %s' % (x+1))\n",
    "        print('loss: ' + str(sess.run(train, {x: x_dataset, y: y_dataset})))\n",
    "        print(\"W: %f, b: %f\" % (sess.run( W ), sess.run( b )) )\n",
    "\n",
    "# evaluate training accuracy\n",
    "ans_W, ans_b, ans_loss = sess.run([W, b, loss], {x: x_dataset, y: y_dataset})\n",
    "print(\"W: %s b: %s loss: %s\"%(ans_W, ans_b, ans_loss))\n",
    "\n",
    "# Tensorflow Estimator 使用例 ------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "num_columns = [tf.feature_column.numeric_column(\"x\",shape=[1])]\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=num_columns)\n",
    "\n",
    "x_dataset = np.array([1.,2.,3.,4.])\n",
    "y_dataset = np.array([0.,-1.,-2.,-3.])\n",
    "x_eval = np.array([2.,5.,8.,1.])\n",
    "y_eval = np.array([-1.01,-4.1,-7.,0.])\n",
    "\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn({\"x\":x_dataset},y_dataset,batch_size=4, num_epochs=None, shuffle=True)\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn({\"x\":x_dataset},y_dataset,batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn({\"x\":x_eval},y_eval,batch_size=4, num_epochs=1000, shuffle=False)\n",
    "ans_train_data = estimator.evaluate(input_fn=train_input_fn)\n",
    "ans_eval_data = estimator.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "print(\"train data vs model: %r\" % ans_train_data)\n",
    "print(\"eval data vs model: %r\" % ans_eval_data)\n",
    "\n",
    "# tensorflow MNIST example -------------------------------------------------------------------------------------\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "#variables\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#model formula\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "#cross entropy\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "#run\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "#accuracy check\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"accuracy:\", sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crawlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleSpider(orders,recipe,output):\n",
    "    import requests\n",
    "    import lxml.html\n",
    "    f = open(orders, 'r')\n",
    "    dump= f.read()\n",
    "    f.close()\n",
    "    l_url = dump.split('\\n')\n",
    "    l_out = []\n",
    "    for url in l_url:\n",
    "        if url != '':\n",
    "            response = requests.get(url)\n",
    "            html = lxml.html.fromstring(response.content)\n",
    "            l_res = []\n",
    "            l_res = html.xpath(recipe)\n",
    "            l_out.extend(l_res)\n",
    "\n",
    "    csv_out = \"\\n\".join(l_out)\n",
    "    f = open(output, 'w')\n",
    "    f.write(csv_out)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "orders = 'target.txt' #1url per line\n",
    "recipe = '//a[contains(@href, \"entry\") and (starts-with(@href,\"http://\") or starts-with(@href,\"https://\"))]/@href'\n",
    "output = 'results.csv'\n",
    "\n",
    "SimpleSpider(orders,recipe,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xpath example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "s_xpathcode = '//a[(starts-with(@href, \"http://\") or starts-with(@href, \"https://\"))' + \\\n",
    "'and contains(@href, ' + s_domain + ') and not(contains(@href, \".jpg\"))]/@href'\n",
    "\n",
    "s_xpathcode = '//*[@class =\"entry-content\"]/p/descendant::text()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# matplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def f2(x, w):\n",
    "    return (x - w) * x * (x + 2) #(A) 関数定義\n",
    "\n",
    "x = np.linspace(-3, 3, 100) # (B) x を100 分割にする\n",
    "\n",
    "\n",
    "plt.plot(x, f2(x, 2), color='black', label='$w=2$') #(C)\n",
    "plt.plot(x, f2(x, 1), color='cornflowerblue',\n",
    "         label='$w=1$') #(D)\n",
    "plt.legend(loc=\"upper left\")           # (E) 凡例表示\n",
    "plt.ylim(-15, 15)                      # (F) y 軸の範囲\n",
    "plt.title('$f_2(x)$')                  # (G) タイトル\n",
    "plt.xlabel('$x$')                      # (H) x ラベル\n",
    "plt.ylabel('$y$')                      # (I) y ラベル\n",
    "plt.grid(True)                         # (J) グリッド\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = df.drop( 'TARGET', axis = 1, inplace=True)\n",
    "f , ax = plt.subplots(figsize = (14,12))\n",
    "plt.title('Correlation of Features- HeatMap',y=1,size=16)\n",
    "sns.heatmap(df.corr(),square = True,  vmax=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split chart area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 3))                 # (A) figure を指定\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5) # (B) グラフの間隔を指定\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)                # (C) グラフ描写の位置を指定\n",
    "    plt.title(i + 1)\n",
    "    plt.plot(x, f2(x, i), 'k')\n",
    "    plt.ylim(-20, 20)\n",
    "    plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "df_draw = df_tmp[df_tmp.Survived==1]\n",
    "df_age = df_draw.iloc[:,2]\n",
    "df_sex = df_draw.iloc[:,6]\n",
    "plt.scatter(df_age,df_sex, color='#cc6699',alpha=0.5)\n",
    "df_draw = df_tmp[df_tmp.Survived==0]\n",
    "df_age = df_draw.iloc[:,2]\n",
    "df_sex = df_draw.iloc[:,6]\n",
    "plt.scatter(df_age,df_sex, color='#6699cc',alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_survived_ratio = [male_survived_ratio, female_survived_ratio]\n",
    "plt.bar([0,1], sex_survived_ratio, tick_label=['male', 'female'], width=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gray scale pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def f3(x0, x1):\n",
    "    r = 2 * x0**2 + x1**2\n",
    "    ans = r * np.exp(-r)\n",
    "    return ans\n",
    "\n",
    "xn = 9\n",
    "x0 = np.linspace(-2, 2, xn)            # (A)\n",
    "x1 = np.linspace(-2, 2, xn)            # (B)\n",
    "y = np.zeros((len(x0), len(x1)))       # (C)\n",
    "for i0 in range(xn):\n",
    "    for i1 in range(xn):\n",
    "        y[i1, i0] = f3(x0[i0], x1[i1]) # (D)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3.5, 3))\n",
    "plt.gray()                     # (A)\n",
    "plt.pcolor(y)                  # (B)\n",
    "plt.colorbar()                 # (C)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D           # (A)\n",
    "\n",
    "xx0, xx1 = np.meshgrid(x0, x1)                    # (B)\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "ax = plt.subplot(1, 1, 1, projection='3d')        # (C)\n",
    "ax.plot_surface(xx0, xx1, y, rstride=1, cstride=1, alpha=0.3,\n",
    "                color='blue', edgecolor='black')  # (D)\n",
    "ax.set_zticks((0, 0.2))                           # (E)\n",
    "ax.view_init(75, -95) # (F)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### air plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = 5 + 25 * np.random.rand(16)\n",
    "\n",
    "X0 = X\n",
    "X1 = 23 * (T / 100)**2 + 2 * np.random.randn(16)\n",
    "\n",
    "Prm_c = [170, 108, 0.2]\n",
    "T = Prm_c[0] - Prm_c[1] * np.exp(-Prm_c[2] * X) \\\n",
    "             + 4 * np.random.randn(16)     \n",
    "\n",
    "def show_data2(ax, x0, x1, t):\n",
    "    for i in range(len(x0)):\n",
    "        ax.plot([x0[i], x0[i]], [x1[i], x1[i]],\n",
    "                [120, t[i]], color='gray')\n",
    "    ax.plot(x0, x1, t, 'o', \n",
    "            color='cornflowerblue', markeredgecolor='black',\n",
    "            markersize=6, markeredgewidth=0.5)\n",
    "    ax.view_init(elev=35, azim=-75)\n",
    "    \n",
    "\n",
    "    \n",
    "plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot(1,1,1,projection='3d')\n",
    "show_data2(ax, X0, X1, T)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
