{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Basics\" data-toc-modified-id=\"Basics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Basics</a></span><ul class=\"toc-item\"><li><span><a href=\"#first-lines\" data-toc-modified-id=\"first-lines-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>first lines</a></span></li><li><span><a href=\"#misc-notes\" data-toc-modified-id=\"misc-notes-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>misc notes</a></span></li><li><span><a href=\"#List\" data-toc-modified-id=\"List-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>List</a></span></li><li><span><a href=\"#Set\" data-toc-modified-id=\"Set-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Set</a></span></li><li><span><a href=\"#Dictionary\" data-toc-modified-id=\"Dictionary-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Dictionary</a></span></li><li><span><a href=\"#Str\" data-toc-modified-id=\"Str-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Str</a></span></li><li><span><a href=\"#Binary\" data-toc-modified-id=\"Binary-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Binary</a></span></li><li><span><a href=\"#Class\" data-toc-modified-id=\"Class-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Class</a></span><ul class=\"toc-item\"><li><span><a href=\"#Class-example-1\" data-toc-modified-id=\"Class-example-1-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Class example 1</a></span></li><li><span><a href=\"#Class-example-2\" data-toc-modified-id=\"Class-example-2-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>Class example 2</a></span></li><li><span><a href=\"#Class-example-3-inherit\" data-toc-modified-id=\"Class-example-3-inherit-1.8.3\"><span class=\"toc-item-num\">1.8.3&nbsp;&nbsp;</span>Class example 3 inherit</a></span></li></ul></li></ul></li><li><span><a href=\"#Load-/-Save\" data-toc-modified-id=\"Load-/-Save-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load / Save</a></span><ul class=\"toc-item\"><li><span><a href=\"#std-input\" data-toc-modified-id=\"std-input-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>std input</a></span></li><li><span><a href=\"#File\" data-toc-modified-id=\"File-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>File</a></span></li><li><span><a href=\"#JSON\" data-toc-modified-id=\"JSON-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>JSON</a></span><ul class=\"toc-item\"><li><span><a href=\"#basics\" data-toc-modified-id=\"basics-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>basics</a></span></li><li><span><a href=\"#Read-CSV-and-parse-JSON\" data-toc-modified-id=\"Read-CSV-and-parse-JSON-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Read CSV and parse JSON</a></span></li></ul></li><li><span><a href=\"#Pandas\" data-toc-modified-id=\"Pandas-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Pandas</a></span></li><li><span><a href=\"#Machine-Learning-models\" data-toc-modified-id=\"Machine-Learning-models-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Machine Learning models</a></span></li><li><span><a href=\"#Google-Colab\" data-toc-modified-id=\"Google-Colab-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Google Colab</a></span><ul class=\"toc-item\"><li><span><a href=\"#mount-google-drive\" data-toc-modified-id=\"mount-google-drive-2.6.1\"><span class=\"toc-item-num\">2.6.1&nbsp;&nbsp;</span>mount google drive</a></span></li><li><span><a href=\"#upload-files\" data-toc-modified-id=\"upload-files-2.6.2\"><span class=\"toc-item-num\">2.6.2&nbsp;&nbsp;</span>upload files</a></span></li><li><span><a href=\"#download-files\" data-toc-modified-id=\"download-files-2.6.3\"><span class=\"toc-item-num\">2.6.3&nbsp;&nbsp;</span>download files</a></span></li></ul></li><li><span><a href=\"#Tensorflow-models\" data-toc-modified-id=\"Tensorflow-models-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Tensorflow models</a></span></li></ul></li><li><span><a href=\"#Crawlers\" data-toc-modified-id=\"Crawlers-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Crawlers</a></span><ul class=\"toc-item\"><li><span><a href=\"#simple-crawler\" data-toc-modified-id=\"simple-crawler-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>simple crawler</a></span></li><li><span><a href=\"#xpath-example\" data-toc-modified-id=\"xpath-example-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>xpath example</a></span></li></ul></li><li><span><a href=\"#Pandas\" data-toc-modified-id=\"Pandas-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Pandas</a></span><ul class=\"toc-item\"><li><span><a href=\"#pandas-general\" data-toc-modified-id=\"pandas-general-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>pandas general</a></span></li><li><span><a href=\"#list-&lt;-&gt;-df\" data-toc-modified-id=\"list-<->-df-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>list &lt;-&gt; df</a></span></li><li><span><a href=\"#drop-/-fill-NaNs\" data-toc-modified-id=\"drop-/-fill-NaNs-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>drop / fill NaNs</a></span><ul class=\"toc-item\"><li><span><a href=\"#substitute-missing-rows-only\" data-toc-modified-id=\"substitute-missing-rows-only-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>substitute missing rows only</a></span></li></ul></li><li><span><a href=\"#delete,-connect,-convert-columns\" data-toc-modified-id=\"delete,-connect,-convert-columns-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>delete, connect, convert columns</a></span></li><li><span><a href=\"#deal-with-dates\" data-toc-modified-id=\"deal-with-dates-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>deal with dates</a></span></li><li><span><a href=\"#count\" data-toc-modified-id=\"count-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>count</a></span></li><li><span><a href=\"#filter\" data-toc-modified-id=\"filter-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>filter</a></span></li><li><span><a href=\"#Grouping\" data-toc-modified-id=\"Grouping-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Grouping</a></span></li><li><span><a href=\"#aggregate\" data-toc-modified-id=\"aggregate-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>aggregate</a></span></li><li><span><a href=\"#progress-bar\" data-toc-modified-id=\"progress-bar-4.10\"><span class=\"toc-item-num\">4.10&nbsp;&nbsp;</span>progress bar</a></span></li><li><span><a href=\"#extract-parts\" data-toc-modified-id=\"extract-parts-4.11\"><span class=\"toc-item-num\">4.11&nbsp;&nbsp;</span>extract parts</a></span></li><li><span><a href=\"#countif\" data-toc-modified-id=\"countif-4.12\"><span class=\"toc-item-num\">4.12&nbsp;&nbsp;</span>countif</a></span></li><li><span><a href=\"#to-OneHot\" data-toc-modified-id=\"to-OneHot-4.13\"><span class=\"toc-item-num\">4.13&nbsp;&nbsp;</span>to OneHot</a></span><ul class=\"toc-item\"><li><span><a href=\"#get-dummies\" data-toc-modified-id=\"get-dummies-4.13.1\"><span class=\"toc-item-num\">4.13.1&nbsp;&nbsp;</span>get dummies</a></span></li><li><span><a href=\"#to-categorical\" data-toc-modified-id=\"to-categorical-4.13.2\"><span class=\"toc-item-num\">4.13.2&nbsp;&nbsp;</span>to categorical</a></span></li></ul></li><li><span><a href=\"#string-categories-to-numbers\" data-toc-modified-id=\"string-categories-to-numbers-4.14\"><span class=\"toc-item-num\">4.14&nbsp;&nbsp;</span>string categories to numbers</a></span><ul class=\"toc-item\"><li><span><a href=\"#factorize\" data-toc-modified-id=\"factorize-4.14.1\"><span class=\"toc-item-num\">4.14.1&nbsp;&nbsp;</span>factorize</a></span></li><li><span><a href=\"#Label-Encoder\" data-toc-modified-id=\"Label-Encoder-4.14.2\"><span class=\"toc-item-num\">4.14.2&nbsp;&nbsp;</span>Label Encoder</a></span></li><li><span><a href=\"#share-same-numbers-between-train-and-test-datasets\" data-toc-modified-id=\"share-same-numbers-between-train-and-test-datasets-4.14.3\"><span class=\"toc-item-num\">4.14.3&nbsp;&nbsp;</span>share same numbers between train and test datasets</a></span></li></ul></li><li><span><a href=\"#cut\" data-toc-modified-id=\"cut-4.15\"><span class=\"toc-item-num\">4.15&nbsp;&nbsp;</span>cut</a></span></li></ul></li><li><span><a href=\"#Matplot\" data-toc-modified-id=\"Matplot-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Matplot</a></span><ul class=\"toc-item\"><li><span><a href=\"#import\" data-toc-modified-id=\"import-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>import</a></span></li><li><span><a href=\"#sample\" data-toc-modified-id=\"sample-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>sample</a></span></li><li><span><a href=\"#split-chart-area\" data-toc-modified-id=\"split-chart-area-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>split chart area</a></span></li><li><span><a href=\"#scatter\" data-toc-modified-id=\"scatter-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>scatter</a></span></li><li><span><a href=\"#bar\" data-toc-modified-id=\"bar-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>bar</a></span></li><li><span><a href=\"#gray-scale-pic\" data-toc-modified-id=\"gray-scale-pic-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>gray scale pic</a></span></li><li><span><a href=\"#3D-plot\" data-toc-modified-id=\"3D-plot-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>3D plot</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mesh\" data-toc-modified-id=\"Mesh-5.7.1\"><span class=\"toc-item-num\">5.7.1&nbsp;&nbsp;</span>Mesh</a></span></li><li><span><a href=\"#air-plot\" data-toc-modified-id=\"air-plot-5.7.2\"><span class=\"toc-item-num\">5.7.2&nbsp;&nbsp;</span>air plot</a></span></li></ul></li></ul></li><li><span><a href=\"#Machine-Learning-Basics\" data-toc-modified-id=\"Machine-Learning-Basics-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Machine Learning Basics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-Datasets\" data-toc-modified-id=\"Split-Datasets-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Split Datasets</a></span></li><li><span><a href=\"#Grid-Search-Cross-Validation\" data-toc-modified-id=\"Grid-Search-Cross-Validation-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Grid Search Cross Validation</a></span></li><li><span><a href=\"#compare-models\" data-toc-modified-id=\"compare-models-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>compare models</a></span></li></ul></li><li><span><a href=\"#Scikit-learn-ML-models\" data-toc-modified-id=\"Scikit-learn-ML-models-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Scikit-learn ML models</a></span></li><li><span><a href=\"#XGBoost-model\" data-toc-modified-id=\"XGBoost-model-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>XGBoost model</a></span></li><li><span><a href=\"#Light-GB-model\" data-toc-modified-id=\"Light-GB-model-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Light GB model</a></span></li><li><span><a href=\"#tensorflow\" data-toc-modified-id=\"tensorflow-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>tensorflow</a></span></li><li><span><a href=\"#Keras\" data-toc-modified-id=\"Keras-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Keras</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#jupyter notebook timer\n",
    "\n",
    "%%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## misc notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PrintのFormating\n",
    "print(\"{0} is {1} years old and he is {2}\".format(\"Mike\",35,\"scary\"))\n",
    "print(\"a: %f, b: %f\" %(a , b))\n",
    "\n",
    "#改行なし Print\n",
    "print(text,end='')\n",
    "\n",
    "#Enumerate\n",
    "x=[\"item1\",\"item2\",\"item3\"]\n",
    "for index,item in enumerate(x):\n",
    "    print(index,\":\",item)\n",
    "\n",
    "y={'key1':'val1', 'key2':'val2', 'key3':'val3'}\n",
    "for index,key in enumerate(y):\n",
    "    print(\"{0} : {1} : {2}\".format(index,key,y[key]))\n",
    "\n",
    "#Function 引数の個数指定なし\n",
    "def biggest_number(*args):\n",
    "    return max(args)\n",
    "X = biggest_number(-10, -5, 5, 10)\n",
    "\n",
    "#Counter\n",
    "from collections import Counter\n",
    "c = Counter([1,1,2,3,3,4,5,5,5])\n",
    "print(c.most_common(3))\n",
    "\n",
    "#Datetime\n",
    "import datetime                       \n",
    "everything = dir(datetime)         \n",
    "print(everything)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_list = list(range(1, 11))\n",
    "backwards = my_list[::-1]\n",
    "\n",
    "l_data = [\"score\",\"40\",\"50\",\"80\"]\n",
    "tmp = l_data.pop(0)\n",
    "n_total = sum(map(lambda x: int(x), l_data))\n",
    "\n",
    "evens_to_50 = [x for x in range(51) if x % 2 == 0]\n",
    "even_squares = [x**2 for x in range(1,11) if x % 2 == 0]\n",
    "threes_and_fives = [x for x in range(1,16) if x %3 == 0 or x % 5 == 0]\n",
    "\n",
    "my_list = list(filter(lambda x: x % 3 == 0,range(16)))\n",
    "squares = list(filter(lambda x:x>=30 and x<= 70,[x**2 for x in range(1,11)]))\n",
    "\n",
    "li = [3, 4, 3, 2, 5, 4]\n",
    "li_unique = list(set(li))\n",
    "\n",
    "l = [1,1,2,3,4,4,5,10]\n",
    "if all(x < 10 for x in l):\n",
    "    print(\"All numbers are less than 10\")\n",
    "if any(x >= 10 for x in l):\n",
    "    print(\"There is a number greater than 10\")\n",
    "\n",
    "animals = [\"aardvark\", \"badger\", \"duck\", \"emu\", \"fennec fox\"]\n",
    "duck_index = animals.index(\"duck\")\n",
    "animals.insert(duck_index,\"cobra\")\n",
    "\n",
    "list_a = [3, 9, 17, 15, 19]\n",
    "list_b = [2, 4, 8, 10, 30, 40, 50, 60, 70, 80, 90]\n",
    "for a, b in zip(list_a, list_b):\n",
    "    print(max(a,b), end=\"  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_a = {7,9,2}\n",
    "set_b = {9,22,2,25}\n",
    "set_c = {9,18,21,5}\n",
    "\n",
    "set_b.add(12)\n",
    "set_b.discard(0)\n",
    "\n",
    "set_union = set_a.union(set_b)\n",
    "set_intersection = set_a.intersection(set_b,set_c)\n",
    "set_differences = set_a.difference(set_b,set_c)\n",
    "set_symmetric_difference = set_a.symmetric_difference(set_b)\n",
    "\n",
    "\n",
    "\n",
    "keys = set()\n",
    "for data in df_train:\n",
    "    data_parsed = json.loads(data)\n",
    "    [keys.add(key) for key in data_parsed.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dict = {\"food\":\"apple\", \"tool\":\"spoon\", \"person\":\"jay\"}\n",
    "print(my_dict.items())\n",
    "print(my_dict.keys())\n",
    "print(my_dict.values())\n",
    "\n",
    "candidates = {\"a\":15, \"b\":25, \"c\":35}\n",
    "winnerVote = int(max(candidates.values()))\n",
    "\n",
    "#Sort\n",
    "orig = {\"1\": 3,  \"3\": 1, \"2\": 2}\n",
    "sortByKey = sorted(orig.items())\n",
    "sortByVal = sorted(orig.items(), key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str = \"abcde\"\n",
    "L = list(str)\n",
    "str2 = \"\".join(L)\n",
    "\n",
    "phrase = \"A bird in the hand...\"\n",
    "for c in phrase:\n",
    "    if c.lower() == 'a':\n",
    "        print('X',end='')\n",
    "    else:\n",
    "        print(c,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 0b1001 + 0b1001\n",
    "y = 0b1001 * 0b1001\n",
    "print (int(\"111\",2))\n",
    "print (int(\"0b100\",2))\n",
    "print (bin(5))\n",
    "print (int(bin(5),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Student:\n",
    "    def __init__(self, name, grade, age):\n",
    "        self.name = name\n",
    "        self.grade = grade\n",
    "        self.age = age\n",
    "    def __repr__(self):\n",
    "        return repr((self.name, self.grade, self.age))\n",
    "l_obj_students = [\n",
    "    Student('john', 'A', 15),\n",
    "    Student('jane', 'B', 12),\n",
    "    Student('dave', 'B', 10),\n",
    "]\n",
    "\n",
    "# ageでsort\n",
    "print(sorted(l_obj_students, key=attrgetter('age'))) \n",
    "\n",
    "# gradeでsort さらにageでsort\n",
    "print(sorted(l_obj_students, key=attrgetter('grade', 'age')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Animal(object):\n",
    "    is_alive = True\n",
    "    num_Animals = 0\n",
    "    AnimalDict={}\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "        Animal.num_Animals += 1\n",
    "        Animal.AnimalDict[name] = self\n",
    "        \n",
    "zebra = Animal(\"Jeffrey\", 2)\n",
    "giraffe = Animal(\"Bruce\", 1)\n",
    "panda = Animal(\"Chad\", 7)\n",
    "\n",
    "print(zebra.name, zebra.age, zebra.is_alive)\n",
    "print(giraffe.name, giraffe.age, giraffe.is_alive)\n",
    "print(panda.name, panda.age, panda.is_alive)\n",
    "print(Animal.num_Animals)\n",
    "print(Animal.AnimalDict)\n",
    "print(Animal.AnimalDict[\"\"Bruce\"\"].age)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class example 3 inherit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Shape(object):\n",
    "    def __init__(self, number_of_sides):\n",
    "        self.number_of_sides = number_of_sides\n",
    "    def __repr__(self):\n",
    "        x = \"\"I have \"\" + str(self.number_of_sides) + \"\" sides\"\"\n",
    "        return x\n",
    "\n",
    "class Triangle(Shape):\n",
    "    def __init__(self,side1, side2, side3):\n",
    "        self.side1 = side1\n",
    "        self.side2 = side2\n",
    "        self.side3 = side3\n",
    "    def __repr__(self):\n",
    "        x = \"\"I have \"\" + str(3) + \"\" sides\"\"\n",
    "        return x\n",
    "\n",
    "myShape = Triangle(1,2,3)\n",
    "print(myShape.side1)\n",
    "print(myShape)\n",
    "\n",
    "yourShape = Shape(4)\n",
    "print(yourShape)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load / Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## std input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "DATA = sys.stdin.read()\n",
    "DATA = DATA.replace(\"\\n\",\"\")\n",
    "DATA = DATA.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "item = \"content\"\n",
    "f = open(\"output.txt\", \"w\")\n",
    "f.write(item + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "dic = {'key1':'val1', 'key2':'val2', 'key3':'val3', 'key4':'val4'}\n",
    "#JSON保存\n",
    "f = open('output.json', 'w')\n",
    "json.dump(dic, f)\n",
    "f.close()\n",
    "#JSON読み込み\n",
    "f = open('output.json', 'r')\n",
    "dic = json.load(f)\n",
    "f.close()\n",
    "\n",
    "s = f.read()\n",
    "dic = json.loads(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV and parse JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "def load_df(csv_path, nrows=None):\n",
    "    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    \n",
    "    df = pd.read_csv(csv_path, \n",
    "                     converters={column: json.loads for column in JSON_COLUMNS}, \n",
    "                     dtype={'fullVisitorId': 'str'}, \n",
    "                     nrows=nrows)\n",
    "    \n",
    "    for column in JSON_COLUMNS:\n",
    "        column_as_df = json_normalize(df[column])\n",
    "        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n",
    "        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n",
    "    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Save\n",
    "df.to_csv('modified.csv')\n",
    "df.to_csv('modified.csv', header=False, index=False)\n",
    "\n",
    "#Load\n",
    "df = pd.read_csv('raw.csv')\n",
    "df = pd.read_csv('raw.csv',index_col=0, dtype='object', nrows=30, header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save\n",
    "\n",
    "import pickle\n",
    "filename = 'xgb_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "#load\n",
    "\n",
    "import pickle\n",
    "filename = 'xgb_model.sav'\n",
    "model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd ../content/gdrive/My Drive/projects/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "csv_data = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from google.colab import files\n",
    "filename = 'model_LGB.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "files.download(filename) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, '../model/test_model')\n",
    "\n",
    "#load\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, '../model/test_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SimpleSpider(orders,recipe,output):\n",
    "    import requests\n",
    "    import lxml.html\n",
    "    f = open(orders, 'r')\n",
    "    dump= f.read()\n",
    "    f.close()\n",
    "    l_url = dump.split('\\n')\n",
    "    l_out = []\n",
    "    for url in l_url:\n",
    "        if url != '':\n",
    "            response = requests.get(url)\n",
    "            html = lxml.html.fromstring(response.content)\n",
    "            l_res = []\n",
    "            l_res = html.xpath(recipe)\n",
    "            l_out.extend(l_res)\n",
    "\n",
    "    csv_out = \"\\n\".join(l_out)\n",
    "    f = open(output, 'w')\n",
    "    f.write(csv_out)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "orders = 'target.txt' #1url per line\n",
    "recipe = '//a[contains(@href, \"entry\") and (starts-with(@href,\"http://\") or starts-with(@href,\"https://\"))]/@href'\n",
    "output = 'results.csv'\n",
    "\n",
    "SimpleSpider(orders,recipe,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xpath example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "s_xpathcode = '//a[(starts-with(@href, \"http://\") or starts-with(@href, \"https://\"))' + \\\n",
    "'and contains(@href, ' + s_domain + ') and not(contains(@href, \".jpg\"))]/@href'\n",
    "\n",
    "s_xpathcode = '//*[@class =\"entry-content\"]/p/descendant::text()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_temp = df.copy() #こうしないと参照コピーになる\n",
    "\n",
    "df.describe()\n",
    "df.info()\n",
    "df.shape()\n",
    "df.isnull().sum()\n",
    "df.notnull().sum()\n",
    "df.corr()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list <-> df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_x = pd.Series(l_x, name = \"ColumnName\")\n",
    "l_x = df['ColumnName'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop / fill NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicated()\n",
    "df['ColumnName'] = df['ColumnName'].notnull() * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### substitute missing rows only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_res_pred = model.predict(X)\n",
    "    df_real_age = df['Age'].fillna(0)\n",
    "    l_mix = []\n",
    "    for i,v in enumerate(df_real_age):\n",
    "        if v != 0:\n",
    "            l_mix.append(v)\n",
    "        else:\n",
    "            l_mix.append(df_res_pred[i])\n",
    "\n",
    "    df_mix = pd.Series(l_mix, name='Age')\n",
    "    df = df.drop('Age',axis = 1)\n",
    "    df = pd.concat((df,df_mix),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## delete, connect, convert columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat((df, df_x), axis = 1)\n",
    "df = df.drop('ColumnName',axis = 1)\n",
    "df = df.drop('ColumnName',axis = 1, inplace=True)\n",
    "\n",
    "series_x[series_x < 0] = 0\n",
    "\n",
    "#convert to number\n",
    "l_toNum = [ 'ColumnOne', 'ColumnTwo', 'ColumnThree']\n",
    "for x in l_toNum:\n",
    "    df[x] = df[x].values.astype(np.int64)\n",
    "\n",
    "l_bools = [ 'ColumnOne', 'ColumnTwo', 'ColumnThree']\n",
    "for x in l_bools:\n",
    "    df[x] = df[x].values.astype(np.int64) * 1\n",
    "\n",
    "#delete unecessary columns\n",
    "l_drop = [ 'ColumnOne', 'ColumnTwo', 'ColumnThree']\n",
    "for x in l_drop:\n",
    "    df.drop(x, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deal with dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fix     \n",
    "df['date'] = pd.to_datetime(df['date'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))\n",
    "\n",
    "#extract\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekday'] = df['date'].dt.weekday\n",
    "df['weekofyear'] = df['date'].dt.weekofyear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df['ColumnName'].value_counts()\n",
    "df = df['ColumnName'].unique()\n",
    "len(df[df['ColumnNameOne'].isnull()==False & (df['ColumnNameTwo'] == 1)])\n",
    "\n",
    "cols_to_drop = [col for col in df.columns if df[col].nunique() == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tmp = df.query(\"ColumnOne==1 and ColumnTwo >= 30\")\n",
    "df_tmp = df.filter(regex='^P')\n",
    "df_tmp = df.select_dtypes(['object'])\n",
    "cat_cols = df.select_dtypes(['object']).columns\n",
    "df_tmp = df.select_dtypes(exclude=np.float)\n",
    "\n",
    "n_oldest = df.Age.max()\n",
    "df_x = df[df.Age==n_oldest]\n",
    "df_x = df[df.Age>=40]\n",
    "\n",
    "df = df[df[0].str.contains('//www.target.com')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupby('GroupingColumn')['TargetColumn'].value_counts().unstack()\n",
    "df.pivot_table(index='GroupingColumnOne', columns='GroupingColumnTwo', values='TargetColumn', aggfunc=np.mean)\n",
    "\n",
    "df = df['ColumnName'].str.split(',', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AGG(df,s_groupby,agg_rules,):\n",
    "\n",
    "    df_agg = df.groupby(s_groupby).agg(aggs_rules)\n",
    "\n",
    "    new_columns = [ k + '_' + agg for k in agg_rules.keys() for agg in aggs[k] ] # 'ColumnOne_min' \n",
    "    df_agg.columns = new_columns    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "s_groupby = 'ColumnToGroupBy'\n",
    "\n",
    "agg_rules = {\n",
    "    'ColumnOne': ['min', 'max'],\n",
    "    'ColumnTwo': ['sum', 'size'],\n",
    "    'ColumnFour': ['sum', 'min', 'max', 'mean', 'median'],\n",
    "    'ColumnFive': ['sum', 'mean', 'median']\n",
    "    'ColumnSix': ['min', 'max', 'mean', 'median', 'var', 'std']\n",
    "}\n",
    "\n",
    "df_new = AGG(df,s_groupby,agg_rules,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df.apply(), df.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "choice = [\"Miss\",\"Mr\",\"Mrs\",\"Master\"]\n",
    "l_title = [list(set(x).intersection(set(choice)))[0] for x in l_names]     # [\"Mr\",\"Johnny\",\"Depp\"] -> \"Mr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## countif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_family = df.Family.apply(lambda x: (df.Family == x).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to OneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummylize(df,n_item):\n",
    "    dum = pd.get_dummies(df[n_item], drop_first = True)\n",
    "    df = pd.concat((df, dum),axis = 1)\n",
    "    df = df.drop(n_item,axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype('int32'),10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## string categories to numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize(df,n_item):\n",
    "    labels, uniques = pd.factorize(df[n_item])\n",
    "    df[n_item] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "num_cat = le.fit_transform(str_categories)\n",
    "df_n_cat = pd.Series(num_cat, name='NumCat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### share same numbers between train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "for col in category_cols:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(df_train[col].values.astype('str')) + list(df_test[col].values.astype('str')))\n",
    "    df_train[col] = lbl.transform(list(df_train[col].values.astype('str')))\n",
    "    df_test[col] = lbl.transform(list(df_test[col].values.astype('str')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.cut(df, [0, 10, 50, 100])\n",
    "pd.cut(df, 4, right=False)\n",
    "counts = pd.cut(df, 3, labels=['S', 'M', 'L']).value_counts()\n",
    "df['Age_bin'] = pd.cut(df['Age'], 5, labels=False)\n",
    "\n",
    "s_cut, bins = pd.cut(df, 4, retbins=True)\n",
    "print(s_cut)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def f2(x, w):\n",
    "    return (x - w) * x * (x + 2) #(A) 関数定義\n",
    "\n",
    "x = np.linspace(-3, 3, 100) # (B) x を100 分割にする\n",
    "\n",
    "\n",
    "plt.plot(x, f2(x, 2), color='black', label='$w=2$') #(C)\n",
    "plt.plot(x, f2(x, 1), color='cornflowerblue',\n",
    "         label='$w=1$') #(D)\n",
    "plt.legend(loc=\"upper left\")           # (E) 凡例表示\n",
    "plt.ylim(-15, 15)                      # (F) y 軸の範囲\n",
    "plt.title('$f_2(x)$')                  # (G) タイトル\n",
    "plt.xlabel('$x$')                      # (H) x ラベル\n",
    "plt.ylabel('$y$')                      # (I) y ラベル\n",
    "plt.grid(True)                         # (J) グリッド\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split chart area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 3))                 # (A) figure を指定\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5) # (B) グラフの間隔を指定\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)                # (C) グラフ描写の位置を指定\n",
    "    plt.title(i + 1)\n",
    "    plt.plot(x, f2(x, i), 'k')\n",
    "    plt.ylim(-20, 20)\n",
    "    plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "df_draw = df_tmp[df_tmp.Survived==1]\n",
    "df_age = df_draw.iloc[:,2]\n",
    "df_sex = df_draw.iloc[:,6]\n",
    "plt.scatter(df_age,df_sex, color='#cc6699',alpha=0.5)\n",
    "df_draw = df_tmp[df_tmp.Survived==0]\n",
    "df_age = df_draw.iloc[:,2]\n",
    "df_sex = df_draw.iloc[:,6]\n",
    "plt.scatter(df_age,df_sex, color='#6699cc',alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sex_survived_ratio = [male_survived_ratio, female_survived_ratio]\n",
    "plt.bar([0,1], sex_survived_ratio, tick_label=['male', 'female'], width=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gray scale pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def f3(x0, x1):\n",
    "    r = 2 * x0**2 + x1**2\n",
    "    ans = r * np.exp(-r)\n",
    "    return ans\n",
    "\n",
    "xn = 9\n",
    "x0 = np.linspace(-2, 2, xn)            # (A)\n",
    "x1 = np.linspace(-2, 2, xn)            # (B)\n",
    "y = np.zeros((len(x0), len(x1)))       # (C)\n",
    "for i0 in range(xn):\n",
    "    for i1 in range(xn):\n",
    "        y[i1, i0] = f3(x0[i0], x1[i1]) # (D)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3.5, 3))\n",
    "plt.gray()                     # (A)\n",
    "plt.pcolor(y)                  # (B)\n",
    "plt.colorbar()                 # (C)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D           # (A)\n",
    "\n",
    "xx0, xx1 = np.meshgrid(x0, x1)                    # (B)\n",
    "\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "ax = plt.subplot(1, 1, 1, projection='3d')        # (C)\n",
    "ax.plot_surface(xx0, xx1, y, rstride=1, cstride=1, alpha=0.3,\n",
    "                color='blue', edgecolor='black')  # (D)\n",
    "ax.set_zticks((0, 0.2))                           # (E)\n",
    "ax.view_init(75, -95) # (F)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### air plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = 5 + 25 * np.random.rand(16)\n",
    "\n",
    "X0 = X\n",
    "X1 = 23 * (T / 100)**2 + 2 * np.random.randn(16)\n",
    "\n",
    "Prm_c = [170, 108, 0.2]\n",
    "T = Prm_c[0] - Prm_c[1] * np.exp(-Prm_c[2] * X) \\\n",
    "             + 4 * np.random.randn(16)     \n",
    "\n",
    "def show_data2(ax, x0, x1, t):\n",
    "    for i in range(len(x0)):\n",
    "        ax.plot([x0[i], x0[i]], [x1[i], x1[i]],\n",
    "                [120, t[i]], color='gray')\n",
    "    ax.plot(x0, x1, t, 'o', \n",
    "            color='cornflowerblue', markeredgecolor='black',\n",
    "            markersize=6, markeredgewidth=0.5)\n",
    "    ax.view_init(elev=35, azim=-75)\n",
    "    \n",
    "\n",
    "    \n",
    "plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot(1,1,1,projection='3d')\n",
    "show_data2(ax, X0, X1, T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l_features = ['Pclass','male','Q','S','TitlesNum','FamilyNum','Cabin','Age','SibSp','Parch','Fare']\n",
    "df_x = df_train[l_features]\n",
    "df_y = df_train.Survived\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def applyGSCV(model, param, X, Y):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    res = GridSearchCV(model, param, cv=3)\n",
    "    res.fit(X, Y)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "models =[]\n",
    "models.append(model_xgb)\n",
    "models.append(model_randForest)\n",
    "models.append(model_MLPC)\n",
    "\n",
    "for model in models:    \n",
    "    res_tmp = model.predict(X_test)\n",
    "    res_mean = mean_squared_error(y_test, res_tmp)\n",
    "\n",
    "    print('mean squared error:{0} '.format(res_mean))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "names = ['LogisticRegression', 'SVC', 'LinearSVC', 'KNeighbors', 'DecisionTree', 'RandomForest', 'MLPClassifier']\n",
    "l_models = []\n",
    "l_models.append((\"LogisticRegression\", LogisticRegression()))\n",
    "l_models.append((\"SVC\", SVC()))\n",
    "l_models.append((\"LinearSVC\", LinearSVC()))\n",
    "l_models.append((\"KNeighbors\", KNeighborsClassifier()))\n",
    "l_models.append((\"DecisionTree\", DecisionTreeClassifier()))\n",
    "l_models.append((\"RandomForest\", RandomForestClassifier()))\n",
    "l_models.append((\"MLPClassifier\", MLPClassifier(solver='lbfgs', random_state=0)))\n",
    "\n",
    "def evaluate_models(df_forAgePred, l_pred, l_models):\n",
    "    results = []\n",
    "    names = []\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_forAgePred[l_pred], df_forAgePred.Age, test_size=0.25)\n",
    "    for name, model in l_models:\n",
    "        model.fit(x_train, y_train)\n",
    "        res_pred = model.predict(x_test)\n",
    "        result = mean_squared_error(y_test, res_pred)\n",
    "        names.append(name)\n",
    "        results.append(result)\n",
    "    return names, results\n",
    "\n",
    "\n",
    "import statistics\n",
    "result_list = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    print('test {0}'.format(i))\n",
    "    name, res = evaluate_models(df_forAgePred, l_pred, l_models)\n",
    "    for x, y in zip(name, res):\n",
    "        result_list.append([x, y])\n",
    "print('mean squared error results by model')\n",
    "\n",
    "for n in names:\n",
    "    r = [i[1] for i in result_list if i[0] == n]\n",
    "    print(n)\n",
    "    print('avg: {0:,.2f} / median: {1:,.2f}'.format(sum(r)/len(r), statistics.median(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_model(X, Y):\n",
    "\n",
    "    import xgboost as xgb\n",
    "    import scipy.stats as st\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "    one_to_left = st.beta(10, 1)\n",
    "    from_zero_positive = st.expon(0, 50)\n",
    "    params = {\n",
    "        \"n_estimators\": st.randint(3, 40),\n",
    "        \"max_depth\": st.randint(3, 40),\n",
    "        \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "        \"colsample_bytree\": one_to_left,\n",
    "        \"subsample\": one_to_left,\n",
    "        \"gamma\": st.uniform(0, 10),\n",
    "        'reg_alpha': from_zero_positive,\n",
    "        \"min_child_weight\": from_zero_positive,\n",
    "    }\n",
    "    xgbreg = xgb.XGBRegressor(nthreads=-1)\n",
    "    rscv = RandomizedSearchCV(xgbreg, params, n_jobs=3)\n",
    "    res = rscv.fit(X, Y)\n",
    "    return res\n",
    "\n",
    "model = xgb_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df_y = df_user_train['totals.transactionRevenue_sum']\n",
    "df_x = df_user_train.drop(['date_min', 'date_max', 'totals.transactionRevenue_sum'], axis=1)\n",
    "df_tgt = df_user_test.drop(['date_min', 'date_max', 'totals.transactionRevenue_sum'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, random_state=1)\n",
    "x_eval, x_valid, y_eval, y_valid = train_test_split(x_test, y_test, random_state=1)\n",
    "\n",
    "lgb_params = {\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 2000,\n",
    "    'num_leaves': 128,\n",
    "    'subsample': 0.2217,\n",
    "    'colsample_bytree': 0.6810,\n",
    "    'min_split_gain': np.power(10.0, -4.9380),\n",
    "    'reg_alpha': np.power(10.0, -3.2454),\n",
    "    'reg_lambda': np.power(10.0, -4.8571),\n",
    "    'min_child_weight': np.power(10.0, 2),\n",
    "    'silent': True\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(x_train, \n",
    "          y_train,\n",
    "          eval_set=[(x_train, y_train),(x_eval, y_eval)],\n",
    "          eval_metric='rmse',\n",
    "          early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tensorflow simple 使用例 --------------------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "\n",
    "# Model parameters (variables of function to be learned via machine learning)\n",
    "W = tf.Variable([0.], dtype=tf.float32)\n",
    "b = tf.Variable([0.], dtype=tf.float32)\n",
    "\n",
    "# Model input and output (sets of data and result)\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model itself\n",
    "linear_model = W*x + b\n",
    "\n",
    "# loss\n",
    "deltas = linear_model - y\n",
    "square_deltas = tf.square(deltas)\n",
    "loss = tf.reduce_sum(square_deltas) # sum of the squares\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_dataset = [1, 2, 3, 4]\n",
    "y_dataset = [0, -1, -2, -3]\n",
    "\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) \n",
    "for x in range(1000):\n",
    "  sess.run(train, {x: x_dataset, y: y_dataset})\n",
    "    if (x+1) % 100 == 0:\n",
    "        print('\\nStep: %s' % (x+1))\n",
    "        print('loss: ' + str(sess.run(train, {x: x_dataset, y: y_dataset})))\n",
    "        print(\"W: %f, b: %f\" % (sess.run( W ), sess.run( b )) )\n",
    "\n",
    "# evaluate training accuracy\n",
    "ans_W, ans_b, ans_loss = sess.run([W, b, loss], {x: x_dataset, y: y_dataset})\n",
    "print(\"W: %s b: %s loss: %s\"%(ans_W, ans_b, ans_loss))\n",
    "\n",
    "# Tensorflow Estimator 使用例 ------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "num_columns = [tf.feature_column.numeric_column(\"x\",shape=[1])]\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=num_columns)\n",
    "\n",
    "x_dataset = np.array([1.,2.,3.,4.])\n",
    "y_dataset = np.array([0.,-1.,-2.,-3.])\n",
    "x_eval = np.array([2.,5.,8.,1.])\n",
    "y_eval = np.array([-1.01,-4.1,-7.,0.])\n",
    "\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn({\"x\":x_dataset},y_dataset,batch_size=4, num_epochs=None, shuffle=True)\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn({\"x\":x_dataset},y_dataset,batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn({\"x\":x_eval},y_eval,batch_size=4, num_epochs=1000, shuffle=False)\n",
    "ans_train_data = estimator.evaluate(input_fn=train_input_fn)\n",
    "ans_eval_data = estimator.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "print(\"train data vs model: %r\" % ans_train_data)\n",
    "print(\"eval data vs model: %r\" % ans_eval_data)\n",
    "\n",
    "# tensorflow MNIST example -------------------------------------------------------------------------------------\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "#variables\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#model formula\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "#cross entropy\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "#run\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "#accuracy check\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"accuracy:\", sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keras simple NNモデル---------------------------------------------------------------------------------------\n",
    "import keras.optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "l_features = ['feat1','feat2','feat3','feat4']\n",
    "N = len(l_features)\n",
    "x_train = df_train[l_features]\n",
    "x_test = df_test[l_features]\n",
    "y_train_bin = to_categorical(df_train['ColumnName'])\n",
    "y_test_bin = to_categorical(df_test['ColumnName'])\n",
    "\n",
    "model_NN = Sequential()\n",
    "model_NN.add(Dense(2,input_dim=N, activation='sigmoid', kernel_initializer='uniform'))\n",
    "model_NN.add(Dense(2,activation='softmax', kernel_initializer='uniform'))\n",
    "sgd = keras.optimizers.SGD(lr = 0.5, momentum = 0.0,decay = 0.0, nesterov = False)\n",
    "model_NN.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model_NN.fit(x_train, y_train_bin, batch_size=100,epochs=1000,verbose=0,validation_data=(x_test, y_test_bin))\n",
    "history.history['acc'] #対trainデータ正確性\n",
    "history.history['val_acc'] #対testデータ正確性\n",
    "\n",
    "score = model_NN.evaluate(X_test, y_test_bin, verbose = 0) \n",
    "print(score[0], score[1]) #score[0]が交差エントロピー誤差、score[1]がテストデータ正答率\n",
    "\n",
    "#Keras mnist 使用例----------------------------------------------------------------------------------\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sequential Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Download MNIST datasets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# show sample data\n",
    "fig = plt.figure(figsize=(9 , 9))\n",
    "for i in range(36):\n",
    "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(x_train[i], cmap='gist_gray')\n",
    "plt.show()\n",
    "\n",
    "# reshape 28*28 pixel data into 784 dim data\n",
    "# convert into float type and normalize pixel data from 0.0 to 1.0\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') /255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') /255\n",
    "\n",
    "# encode label data into \"one-hot\"\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train.astype('int32'), 10)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test.astype('int32'), 10)\n",
    "\n",
    "# select Sequiential model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st layer : fully connected layer(output:512)\n",
    "# only first layer needs to define input_shape\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# use Dropout regularization rate to avoid overfitting\n",
    "# Randomly ignoring connections between layers\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 2nd layer : fully connected layer(output:512)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 3rd layer : fully connected layer(output:10)\n",
    "# acrivation methods: softmax, which squashes the outputs of each unit to be between 0 and 1.(often used in the final layer)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Set definitions for traning\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "\n",
    "# Excute training for 20(epochs) times\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# plot results\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Keras mnist 使用例 2 ----------------------------------------------------------------------------------\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "img_rows, img_cols = 28, 28\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "y_train = y_train.astype('int32')\n",
    "y_test = y_test.astype('int32')\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test =  keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "318px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
